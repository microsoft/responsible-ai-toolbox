{
  "Core": {
    "ExpandableText": {
      "SeeMore": "See more",
      "SeeLess": "See less"
    }
  },
  "Interpret": {
    "calloutTitle": "Click for info",
    "selectPoint": "Select a point to see its local explanation",
    "_selectPoint.comment": "Prompts the user to select a point",
    "defaultClassNames": "Class {0}",
    "_defaultClassNames.comment": " models that output classes have this as the default class names",
    "defaultFeatureNames": "Feature {0}",
    "_defaultFeatureNames.comment": "the default column names",
    "absoluteAverage": "Average of absolute value",
    "_absoluteAverage.comment": "https://en.wikipedia.org/wiki/Norm_(mathematics) Absolute value norm",
    "predictedClass": "Predicted class",
    "_predictedClass.comment": "Norm based on the predicted class rather than absolute value as above",
    "datasetExplorer": "Dataset explorer",
    "dataExploration": "Dataset exploration",
    "_dataExploration.comment": "Label for tab showing scatter plot of dataset and predictions",
    "aggregateFeatureImportance": "Aggregate feature importance",
    "_aggregateFeatureImportance.comment": "tab label for view of aggregated (summed and averaged) feature importances",
    "globalImportance": "Global importance",
    "_globalImportance.comment": "Label for tab showing bar chart of importance of features at a global level",
    "explanationExploration": "Explanation exploration",
    "individualImportance": "Individual feature importance",
    "individualAndWhatIf": "Individual feature importance & what-if",
    "_individualAndWhatIf.comment": "tab label for feature importances of single rows, and 'what if?' which allows seeing hypothetical results",
    "_explanationExploration.comment": "Label for tab showing scatter plot of dataset and importance of features data",
    "summaryImportance": "Summary importance",
    "_summaryImportance.comment": "Label showing all importance of feature data points in scatter plot",
    "featureImportance": "Feature importance",
    "_featureImportance.comment": "Label for feature importance ",
    "featureImportanceOf": "Feature importance of {0}",
    "perturbationExploration": "Perturbation exploration",
    "_perturbationExploration.comment": "Label for local tab allowing the user to change parameters on a selected point",
    "localFeatureImportance": "Local feature importance",
    "_localFeatureImportance.comment": "Label for local tab showing feature importance of selected point",
    "ice": "ICE",
    "_ice.comment": "Label for tab showing https://christophm.github.io/interpretable-ml-book/ice.html",
    "clearSelection": "Clear selection",
    "feature": "Feature:",
    "intercept": "Intercept",
    "_intercept.comment": "The label for the linear intercept, the bias value",
    "modelPerformance": "Model performance",
    "_modelPerformance.comment": "tab label to see how well a model predicted values when compared to true",
    "ExplanationScatter": {
      "dataLabel": "Data : {0}",
      "_dataLabel.comment": "prepend in front of column names",
      "importanceLabel": "Importance : {0}",
      "_importanceLabel.comment": "prepend in front of feature importance of column name",
      "predictedY": "Predicted Y",
      "_predictedY.comment": "predicted output",
      "index": "Index",
      "_index.comment": "the index value (an integer of the row number)",
      "dataGroupLabel": "Data",
      "output": "Output",
      "probabilityLabel": "Probability : {0}",
      "_probabilityLabel.comment": "Probability prefix for all classes in a multiclass problem",
      "trueY": "True Y",
      "_trueY.comment": "The true value to be predicted",
      "class": "class: ",
      "_class.comment": "label for predicted class",
      "xValue": "X value:",
      "_xValue.comment": "label for x value dropdown",
      "yValue": "Y value:",
      "_yValue.comment": "label for y value dropdown",
      "colorValue": "Color:",
      "_colorValue.comment": "label for selecting color value",
      "count": "Count",
      "_count.comment": "the default axis is the count of items"
    },
    "CrossClass": {
      "label": "Cross-class weighting:",
      "_label.comment": "label for dropdown allowing user to select how importance across multiple output classes is aggregated",
      "info": "Information on cross-class calculation",
      "_info.comment": "tooltip on hover",
      "overviewInfo": "Multiclass models generate an independent feature importance vector for each class. Each class's feature importance vector demonstrates which features made a class more likely or less likely. You can select how the weights of the per-class feature importance vectors are summarized into a single value:",
      "_overview.comment": "explains absolute value weights",
      "absoluteValInfo": "Average of absolute value: Shows the sum of the feature's importance across all possible classes, divided by number of classes",
      "predictedClassInfo": "Predicted class: Shows the feature importance value for a given point's predicted class",
      "_predictedClassInfo.comment": "explains predicted class weight",
      "enumeratedClassInfo": "Enumerated class names: Shows only the specified class's feature importance values across all data points.",
      "_enumeratedClassInfo.comment": "explains the weights for selecting a single class",
      "close": "Close",
      "crossClassWeights": "Cross class weights"
    },
    "AggregateImportance": {
      "scaledFeatureValue": "Scaled feature value",
      "_scaledFeatureValue.comment": "The chart shows all data in a color scale (normalized to 0 - 1). This is the label for the color bar",
      "low": "Low",
      "_low.comment": "The low end of the color bar",
      "high": "High",
      "_high.comment": "label for the high end of the color bar",
      "featureLabel": "Feature: {0}",
      "_featureLabel.comment": "Prefix to the feature name",
      "valueLabel": "Feature value: {0}",
      "_valueLabel.comment": "prefix to the feature value",
      "importanceLabel": "Importance: {0}",
      "_importanceLabel.comment": "prefix to the feature importance",
      "predictedClassTooltip": "Predicted class: {0}",
      "_predictedClassTooltip.comment": "prefixed in front of the output class names predicted by the model",
      "trueClassTooltip": "True class: {0}",
      "_trueClassTooltip.comment": "prefixed in front of the true class labels",
      "predictedOutputTooltip": "Predicted output: {0}",
      "_predictedOutputTooltip.comment": "prefixed in front of the output in a regression model (numeric, no classes)",
      "trueOutputTooltip": "True output: {0}",
      "_trueOutputTooltip.comment": "prefixed in front of the true value in a regression model (numeric, no classes)",
      "topKFeatures": "Top K Features:",
      "_topKFeatures.comment": "Label for slider to show only the top (k) most important features, where the slider is used to set the value of k",
      "topKInfo": "How top k is calculated",
      "predictedValue": "Predicted value",
      "_predictedValue.comment": "Label for dropdown option, group data by the predicted value from the model (numeric values)",
      "predictedClass": "Predicted class",
      "_predictedClass.comment": "Label for dropdown option, group data by predicted class from model",
      "trueValue": "True value",
      "_trueValue.comment": "label for dropdown option, group data by true value (numeric)",
      "trueClass": "True class",
      "_trueClass.comment": "label for dropdown, group data by true class",
      "noColor": "None",
      "_noColor.comment": "label for dropdown, do not apply any grouping",
      "tooManyRows": "The provided dataset is larger than this chart can support",
      "_tooManyRows.comment": "error message if the dataset is too large to visualize"
    },
    "BarChart": {
      "classLabel": "Class: {0}",
      "_classLabel.comment": "Prefix for class",
      "sortBy": "Sort by",
      "_sortBy.comment": "prompt for setting how values are sorted",
      "noData": "No data",
      "_noData.comment": "Error message for no applicable data",
      "absoluteGlobal": "Absolute global",
      "_absoluteGlobal.comment": "sorting option, sort by the absolute value of the importance of all datapoints",
      "absoluteLocal": "Absolute local",
      "_absoluteLocal.comment": "sorting option, sort by the absolute value of the importance for the single selected point",
      "calculatingExplanation": "Calculating explanation",
      "_calculatingExplanation.comment": "loading message"
    },
    "IcePlot": {
      "numericError": "Must be numeric",
      "_numericError.comment": "error message if non-numeric characters typed",
      "integerError": "Must be an integer",
      "_integerError.comment": "error message if non-integer values typed by user",
      "prediction": "Prediction",
      "_prediction.comment": "Prediction label for y-axis",
      "predictedProbability": "Predicted probability",
      "_predictedProbability.comment": "predicted probability label for y-axis",
      "predictionLabel": "Prediction: {0}",
      "_predictionLabel.comment": "prediction hover prefix",
      "probabilityLabel": "Probability: {0}",
      "_probabilityLabel.comment": "probability hover prefix",
      "noModelError": "Please provide an operationalized model to explore predictions in ICE plots.",
      "_noModelError.comment": "error message for no model present",
      "featurePickerLabel": "Feature:",
      "_featurePicker.comment": "feature dropdown label",
      "minimumInputLabel": "Minimum:",
      "_minimumInputLabel.comment": "Set minimum bounds label",
      "maximumInputLabel": "Maximum:",
      "_maximumInputLabel.comment": "set maximum bounds label",
      "stepInputLabel": "Steps:",
      "_stepInputLabel.comment": "number of samples to include between minimum and maximum (integer)",
      "loadingMessage": "Loading data...",
      "_loadingMessage.comment": "loading message",
      "submitPrompt": "Submit a range to view an ICE plot",
      "_submitPrompt.comment": "prompt to user giving instructions to enter a numeric range",
      "topLevelErrorMessage": "Error in parameter",
      "_topLevelErrorMessage.comment": "error message for any parameter issue",
      "errorPrefix": "Error encountered: {0}",
      "_errorPrefix.comment": "prefix in front of external error"
    },
    "PerturbationExploration": {
      "loadingMessage": "Loading...",
      "perturbationLabel": "Perturbation:",
      "_perturbationLabel.comment": "Perturbation (ie. the user has made a set of small changes to the original data -- a perturbation. This is the label for the resulting prediction)"
    },
    "PredictionLabel": {
      "predictedValueLabel": "Predicted value : {0}",
      "_predictionValueLabel.comment": "label for prediction of numeric values",
      "predictedClassLabel": "Predicted class : {0}",
      "_predictedClassLabel.comment": "label for prediction of class value"
    },
    "Violin": {
      "groupNone": "No grouping",
      "_groupName.comment": "Do not group data option",
      "groupPredicted": "Predicted Y",
      "_groupPredicted.comment": "option to group data by predicted class",
      "groupTrue": "True Y",
      "_groupTrue.comment": "option to group data by true class",
      "groupBy": "Group by",
      "_groupBy.comment": "Group by prompt for dropdown to select how data should be grouped"
    },
    "FeatureImportanceWrapper": {
      "chartType": "Chart type:",
      "_chartType.comment": "label for dropdown to select chart format",
      "violinText": "Violin",
      "_violinText.comment": "a violin plot https://en.wikipedia.org/wiki/Violin_plot",
      "barText": "Bar",
      "_barText.comment": "a bar plot ",
      "boxText": "Box",
      "_boxText.comment": "a box plot https://en.wikipedia.org/wiki/Box_plot",
      "beehiveText": "Swarm",
      "_beehiveText.comment": "A swarm plot (its like a scatter plot with categorical x axis with dithering, see examples https://seaborn.pydata.org/generated/seaborn.swarmplot.html)",
      "globalImportanceExplanation": "Global feature importance is calculated by averaging the absolute value of the feature importance of all points (L1 normalization). ",
      "_globalImportanceExplanation.comment": "explains how global feature importance is calculated ",
      "multiclassImportanceAddendum": "All points are included in calculating a feature's importance for all classes, no differential weighting is used. So a feature that has large negative importance for many points predicted to not be of 'Class A' will greatly increase that feature's 'Class A'  importance.",
      "_multiclassImportanceAddendum.comment": "explains how global importance is calculated for each class in a multiclass case."
    },
    "Filters": {
      "equalComparison": "Equal to",
      "_equalComparison.comment": "filter for rows that are exactly equal",
      "greaterThanComparison": "Greater than",
      "_greaterThanComparison.comment": "filter for rows that are greater than a value",
      "greaterThanEqualToComparison": "Greater than or equal to",
      "_greaterThanEqualToComparison.comment": "filter for rows that are greater than or equal to a value",
      "lessThanComparison": "Less than",
      "_lessThanComparison.comment": "filter for rows that are less than a value",
      "lessThanEqualToComparison": "Less than or equal to",
      "_lessThanEqualToComparison.comment": "filter for rows that are less than or equal to a value",
      "inTheRangeOf": "In the range of",
      "_inTheRangeOf.comment": "filter for rows that are between two values",
      "categoricalIncludeValues": "Included values:",
      "_categoricalIncludeValues.comment": "filter to selected categories",
      "numericValue": "Value",
      "_numericValue.comment": "the value to compare to in greater/less than or equal to filter",
      "numericalComparison": "Operation",
      "_numericalComparison.comment": "label for dropdown containing [greater than, less than, equal to]",
      "minimum": "Minimum",
      "maximum": "Maximum",
      "min": "Min: {0}",
      "max": "Max: {0}",
      "uniqueValues": "# of unique values: {0}",
      "_uniqueValues.comment": "the number of unique values for a selected categorical filter"
    },
    "Columns": {
      "regressionError": "Regression error",
      "_regressionError.comment": "true value minus predicted value is regression error",
      "error": "Error",
      "classificationOutcome": "Classification outcome",
      "_classificationOutcome.comment": "Whether the prediction from the model matched the true value",
      "truePositive": "True positive",
      "trueNegative": "True negative",
      "falsePositive": "False positive",
      "falseNegative": "False negative",
      "dataset": "Dataset",
      "predictedProbabilities": "Prediction probabilities",
      "none": "Count",
      "_none.comment": "option to not have data on this axis, instead just counts number of points"
    },
    "WhatIf": {
      "closeAriaLabel": "Close",
      "defaultCustomRootName": "Copy of row {0}",
      "_defaultCustomRootName.comment": "default prefix for a hypothetical point made by copying another point",
      "filterFeaturePlaceholder": "Search features",
      "_filterFeaturePlaceholder.comment": "placeholder in search box for searching for features by name"
    },
    "Cohort": {
      "cohort": "Cohort",
      "_cohort.comment": "a subset of the data is called a cohort",
      "defaultLabel": "All data"
    },
    "GlobalTab": {
      "collapsedHelperText": "Explore the top k important features that impact your overall model predictions.",
      "helperText": "Use the slider to show descending feature importances . Select up to three cohorts to see their feature importances side by side. Click on any of the features in the graph to see a density plot below of how values of the selected feature affect prediction.",
      "_helperText.comment": "paragraph summarizing the view on this page and available actions",
      "topAtoB": "Top {0}-{1} features",
      "_topAtoB.comment": "label on a slider, will tell user the index of the features they are currently seeing, like Top 5-10 features",
      "datasetCohorts": "Dataset cohorts",
      "_datasetCohorts.comment": "label for dropdown allowing users to select what cohorts to view",
      "legendHelpText": "Toggle cohorts on and off in the plot by clicking on the legend items.",
      "_legendHelperText.comment": "explanatory text on what actions can be done on a list of cohorts",
      "sortBy": "Sort by",
      "_sortBy.comment": "prompt for setting how values are sorted",
      "viewDependencePlotFor": "View dependence plot for:",
      "_viewDependencePlotFor.comment": "label for dropdown to select feature to be shown in a dependence plot (a kind of graph)",
      "datasetCohortSelector": "Select a dataset cohort",
      "_datasetCohortSelector.comment": "label for selecting single cohort",
      "aggregateFeatureImportance": "Aggregate feature importance",
      "_aggregateFeatureImportance.comment": "graph label for aggregated (summed and averaged) feature importances",
      "missingParameters": "This tab requires the local feature importance parameter be supplied.",
      "_missingParameters.comment": "Show a message if the required feature importance parameter is not provided",
      "weightOptions": "Class importance weights",
      "_weightOptions.comment": "Weight how importance values are averaged https://en.wikipedia.org/wiki/Weighted_arithmetic_mean",
      "dependencePlotTitle": "Dependence plots",
      "dependencePlotHelperText": "This dependence plot shows the relationship between the value of a feature to the corresponding importance of the feature across a cohort.",
      "dependencePlotFeatureSelectPlaceholder": "Select feature",
      "datasetRequired": "Dependence plots require the evaluation dataset and local feature importance array."
    },
    "CohortBanner": {
      "details": "Details",
      "name": "Name",
      "dataStatistics": "Data statistics",
      "_dataStatistics.comment": "label for section containing statistics about the dataset",
      "datapoints": "{0} datapoints",
      "_datapoints.comment": "formatted string of the number of datapoints in the dataset",
      "features": "{0} features",
      "_features.comment": "formatted string of the number of features (columns) in a dataset",
      "filters": "{0} filters",
      "_filters.comment": "the number of filters that define a cohort",
      "binaryClassifier": "Binary classifier",
      "_binaryClassifier.comment": "a model that predicts true or false is a binary classifier, this is the label",
      "regressor": "Regressor",
      "_regressor.comment": "a class of models that output a numeric score, name is derived from statistical regression",
      "multiclassClassifier": "Multiclass classifier",
      "_multiclassClassifier.comment": "models that output a category, with more than two categories",
      "datasetCohorts": "Dataset cohorts",
      "_datasetCohorts.comment": "a subset of the original data, defined by filtering the data. This is the label for presenting all cohorts the user created",
      "editCohort": "Edit cohort",
      "_editCohort.comment": "button text to edit the filters defining an existing cohort",
      "duplicateCohort": "Duplicate",
      "_duplicateCohort.comment": "button text to copy an existing cohort",
      "addCohort": "New cohort",
      "_addCohort.comment": "button text to create a new cohort",
      "copy": " copy",
      "_copy.comment": "suffix attached to name of cohort created by copying other cohort, by default."
    },
    "ModelPerformance": {
      "collapsedHelperText": "Evaluate the performance of your model by exploring the distribution of your prediction values and the values of your model performance metrics.",
      "helperText": "You can further investigate your model by looking at a comparative analysis of its performance across different cohorts or subgroups of your dataset. Select filters along y-value and x-value to cut across different dimensions. Select the gear in the graph to change graph type.",
      "_helperText.comment": "explains the view on this page and what actions can be taken",
      "modelStatistics": "Model statistics",
      "_modelStatistics.comment": "label for area listing statistics about model prediction",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "_cohortPickerLabel.comment": "label for single-select dropdown to pick a cohort to view",
      "missingParameters": "This tab requires the array of predicted values from the model be supplied.",
      "_missingPArameters.comment": "Show a message if the required prediction array not provided",
      "missingTrueY": "Model performance statistics require the true outcomes be provided in addition to the predicted outcomes",
      "_missingTrueY.comment": "Show message if true Y values are not provided, since statistics require the true value"
    },
    "Charts": {
      "yValue": "Y-value",
      "_yValue.comment": "label for y value button on chart",
      "numberOfDatapoints": "Number of datapoints",
      "_numberOfDatapoints.comment": "some charts will always show the count of the number of rows in a group, this is the axis label on the chart",
      "xValue": "X-value",
      "_xValue.comment": "label for x value button on chart",
      "rowIndex": "Row index",
      "_rowIndex.comment": "the index of a row in a dataset",
      "featureImportance": "Feature importance",
      "countTooltipPrefix": "Count: {0}",
      "_countTooltipPrefix.comment": "on hover, show the prefix followed by the total number of points",
      "count": "Count",
      "featurePrefix": "Feature",
      "_featurePrefix.comment": "label shown before feature name in tooltip",
      "importancePrefix": "Importance",
      "_importance.comment": "label shown before importance value in tooltip",
      "cohort": "Cohort",
      "howToRead": "How to read this chart"
    },
    "DatasetExplorer": {
      "collapsedHelperText": "Learn about the over/underpresentation in your dataset.",
      "helperText": "Create dataset cohorts to the left to analyze statistics along filters such as predicted outcome, dataset features and error groups. Use the x-axis and color selectors to further slice your dataset into more dimensions.",
      "_helperText.comment": "paragraph summarizing the view on this page and what actions the user can take",
      "colorValue": "Color value",
      "_colorValue.comment": "label on button to set how data is mapped to color in chart",
      "individualDatapoints": "Individual datapoints",
      "_individualDatapoints.comment": "configuration option to view chart with each individual point, opposed to chart summing/averaging points",
      "aggregatePlots": "Aggregate plots",
      "_aggregatePlots.comment": "configuration option to view plots that sum or average points, opposed to sowing each point on its own",
      "chartType": "Chart type",
      "_chartType.comment": "label for dropdown to select chart format",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "_missingPArameters.comment": "Show a message if the required dataset parameter is not provided",
      "noColor": "None",
      "_noColor.comment": "placeholder text when no color axis picked"
    },
    "DependencePlot": {
      "featureImportanceOf": "Feature importance of",
      "_featureImportanceOf.comment": "axis label on chart showing the importance of a selected feature (column)",
      "placeholder": "Click on a feature on the bar chart above to show its dependence plot",
      "_placeholder.comment": "placeholder text explaining how to activate this chart, by clicking on a neighboring chart."
    },
    "WhatIfTab": {
      "helperText": "Select an individual datapoint by cllicking on a datapoint in the scatterplot to view its local feature importance values below and feature values in the panel on the right.",
      "_helperText.comment": "explains what is shown in this tab and what actions are available",
      "panelPlaceholder": "A model is required to make predictions for new data points.",
      "_panelPlaceholder.comment": "message shown to user when they did not give a model as inputs",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "_cohortPickerLabel.comment": "label for single-select dropdown to pick a cohort to view",
      "scatterLegendText": "Toggle datapoints on and off in the plot by clicking on the legend items.",
      "_scatterLegendText.comment": "describes actions possible on the legend items in the chart",
      "realPoint": "Real datapoints",
      "_realPoint.comment": "label above the list of points from the dataset",
      "noneSelectedYet": "None selected yet",
      "_noneSelectedYet.comment": "placeholder if no points are selected, where list of points would go",
      "dataPointInfo": "Data point info",
      "whatIfDatapoints": "What-If datapoints",
      "_whatIfDatapoints.comment": "label above the list of what if (hypothetical) points",
      "noneCreatedYet": "None created yet",
      "_noneCreatedYet.comment": "placeholder if no hypothetical points have been created, goes where list of points would be",
      "showLabel": "Show:",
      "_showLabel.comment": "label on button to pick what additional chart to show",
      "localFeatureImportanceForPoint": "Local feature importance of selected data points",
      "featureImportancePlot": "Feature importance plot",
      "_featureImportancePlot.comment": "name of a plot type showing the importance of each feature in making a decision",
      "icePlot": "Individual conditional expectation (ICE) plot",
      "_icePlot.comment": "name of a plot type named Individual Conditional Expectation (ICE)",
      "featureImportanceLackingParameters": "Provide local feature importances to see how each feature impacts individual predictions.",
      "_featureImportanceLackingParameters.comment": "placeholder if no local feature importances passed in",
      "featureImportanceGetStartedText": "Select a point to see its local importance",
      "_featureImportanceGetStartedText.comment": "placeholder to prompt user to click point in neighboring chart to view this chart",
      "iceLackingParameters": "ICE plots require an operationalized model to make predictions for hypothetical datapoints.",
      "_iceLackingParameters.comment": "placeholder text if a required parameter is not passed in, cannot show ICE plot if no model passed in",
      "IceGetStartedText": "Select a point or create a What-If point to view ICE plots",
      "_IceGetStartedText.comment": "placeholder to prompt user to click point in neighboring chart to see this chart",
      "whatIfDatapoint": "What-If datapoint",
      "_whatIfDatapoint.comment": "header text for area where user writes a hypothetical point's values",
      "whatIfHelpText": "Select a point on the plot or manually enter a known datapoint index to perturb and save as a new What-If point.",
      "_whatIfHelpText.comment": "describe how to create a what-if hypothetical row",
      "notAvailable": "What-If is currently not supported in studio. Run this widget in a jupyter notebook to enable What-If.",
      "indexLabel": "Data index",
      "_indexLabel.comment": "label for dropdown for selecting a row by index value",
      "rowLabel": "Row {0}",
      "_rowLabel.comment": "the label for a selected row, with its index number appended",
      "whatIfNameLabel": "What-If datapoint name",
      "_whatIfNameLabel.comment": "label above text field where user can name their what-if hypothetical point",
      "featureValues": "Feature values",
      "_featureValues.comment": "header above the list of feature names (column names)",
      "predictedClass": "Predicted class: ",
      "_predictedClass.comment": "the predicted class for a row",
      "predictedValue": "Predicted value: ",
      "_predictedValue.comment": "the predicted value for a row",
      "probability": "Probability: ",
      "_probability.comment": "the probability that the predicted class is correct",
      "trueClass": "True class: ",
      "_trueClass.comment": "prefix to actual label",
      "trueValue": "True value: ",
      "trueValue.comment": "prefix to actual label for regression",
      "newPredictedClass": "New predicted class: ",
      "_newPredictedClass.comment": "the prediction after the user changed features",
      "newPredictedValue": "New predicted value: ",
      "_newPredictedValue.comment": "the prediction after the user changed features",
      "newProbability": "New probability: ",
      "_newProbability.comment": "the probability for the new prediction",
      "saveAsNewPoint": "Save as new point",
      "_saveAsNewPoint.comment": "button to save hypothetical point",
      "saveChanges": "Save changes",
      "_saveChanges.comment": "button text to save changes made to a row",
      "loading": "Loading...",
      "_loading.comment": "loading message while prediction is made",
      "classLabel": "Class: {0}",
      "_classLabel.comment": "Prefix for class",
      "minLabel": "Min",
      "_minLabel.comment": "minimum (small space available)",
      "maxLabel": "Max",
      "_maxLabel.comment": "maximum (small space available)",
      "stepsLabel": "Steps",
      "_stepsLabel.comment": "number of increments to use between minimum and maximum",
      "disclaimer": "Disclaimer: These are explanations based on many approximations and are not the \"cause\" of predictions. Without strict mathematical robustness of causal inference, we do not advise users to make real-life decisions based on this tool.",
      "_disclaimer.comment": "the tool should not be liable for any bad predictions",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "_missingParameters.comment": "Show a message if the required dataset parameter is not provided",
      "selectionLimit": "Maximum of 3 selected points",
      "_selectionLimit.comment": "A user can only select 3 points from a chart at a time, this message is displayed if they click a 4th",
      "classPickerLabel": "Class",
      "tooltipTitleMany": "Top {0} predicted classes",
      "_tooltipTitleMany.comment": "placeholder is the number of classes shown",
      "whatIfTooltipTitle": "What-If predicted classes",
      "tooltipTitleFew": "Predicted classes",
      "probabilityLabel": "Probability",
      "deltaLabel": "Delta",
      "_deltaLabel.comment": "represents the change in a value",
      "nonNumericValue": "Value should be numeric",
      "icePlotHelperText": "ICE plots demonstrate how the selected datapoint's prediction values change along a range of feature values between a minimum and maximum value."
    },
    "CohortEditor": {
      "selectFilter": "Select filter",
      "_selectFilter.comment": "prompt to select an attribute to filter on",
      "TreatAsCategorical": "Treat as categorical",
      "_TreatAsCategorical.comment": "a checkbox label to treat integers as categories instead of as numbers",
      "addFilter": "Add filter",
      "_addFilter.comment": "button text to add the current settings as a new filter",
      "addedFilters": "Added filters",
      "_addedFilters.comment": "header above the list of filters that have been saved",
      "noAddedFilters": "No filters added yet",
      "_noAddedFilters.comment": "placeholder text when no filters are included",
      "defaultFilterState": "Select a filter to add parameters to your dataset cohort.",
      "_defaultFilterState.comment": "placeholder text prompting user to start making a filter",
      "cohortNameLabel": "Dataset cohort name",
      "_cohortNameLabel.comment": "label for text filed where user adds name of a cohort (subset)",
      "cohortNamePlaceholder": "Name your cohort",
      "_cohortNamePlaceholder.comment": "placeholder for cohort name",
      "save": "Save",
      "delete": "Delete",
      "cancel": "Cancel",
      "cohortNameError": "Missing cohort name",
      "_cohortNameError.comment": "error message if required name is missing",
      "placeholderName": "Cohort {0}",
      "_placeholderName.comment": "starting name for a new cohort",
      "cancelTitle": "Cancel cohort",
      "cancelNewCohort": "Are you sure you want to cancel creating a new cohort and go back?",
      "cancelExistingCohort": "Are you sure you want to cancel editing cohort and go back?",
      "cancelYes": "Yes",
      "cancelNo": "No"
    },
    "AxisConfigDialog": {
      "select": "Select",
      "_select.comment": "label above dropdown to promp user to pick a feature",
      "ditherLabel": "Should dither",
      "_ditherLabel.comment": "checkbox label for if small random changes should be added to numbers to more easily see large clusters",
      "selectFilter": "Select your axis value",
      "_selectFilter.comment": "label on dropdown to pick value for charting",
      "selectFeature": "Select feature",
      "_selectFeature.comment": "dropdown label to select feature (column) for charting",
      "binLabel": "Apply binning to data",
      "_binLabel.comment": "group all values into a fixed number of groups (bins)",
      "TreatAsCategorical": "Treat as categorical",
      "_TreatAsCategorical.comment": "a checkbox label to treat integers as categories instead of as numbers",
      "numOfBins": "Number of bins",
      "_numberOfBins.comment": "the number of groups (bins) to place all values into",
      "groupByCohort": "Group by cohort",
      "_groupByCohort.comment": "if user selects to group by cohort, no further parameters to set, just show a message to fill space",
      "selectClass": "Select class",
      "_selectClass.comment": "label for dropdown listing all classes",
      "countHelperText": "A histogram of the number of points"
    },
    "ValidationErrors": {
      "predictedProbability": "Predicted probability",
      "predictedY": "Predicted Y",
      "evalData": "Evaluation dataset",
      "globalFeatureImportance": "Global feature importance",
      "localFeatureImportance": "Local feature importance",
      "inconsistentDimensions": "Inconsistent dimensions. {0} has dimensions {1}, expected {2}",
      "_inconsistentDimensions.comment": "Raise warning if arguments passed in have different sizes, listing dimensions of both mismatching pieces.",
      "notNonEmpty": "{0} input not a non-empty array",
      "varyingLength": "Inconsistent dimensions. {0} has elements of varying length",
      "notArray": "{0} not an array. Expected array of dimension {1}",
      "errorHeader": "Some input parameters were inconsistent and will not be used: ",
      "datasizeWarning": "The evaluation dataset is too large to be effectively displayed in some charts, please add filters to decrease the size of the cohort. ",
      "datasizeError": "The selected cohort is too large, please add filters to decrease the size of the cohort.",
      "addFilters": "Add filters"
    },
    "FilterOperations": {
      "equals": " = {0}",
      "lessThan": " < {0}",
      "greaterThan": " > {0}",
      "lessThanEquals": " <= {0}",
      "greaterThanEquals": " >= {0}",
      "includes": " includes {0} ",
      "excludes": " excludes {0} ",
      "_includes.comment": "tooltip label for a filter with included values",
      "inTheRangeOf": "[ {0} ]",
      "overflowFilterArgs": "{0} and {1} others",
      "_overflowFilterArgs.comment": "first placeholder is the first one or two items in a long list, the second placeholder is the count of remaining items"
    },
    "Statistics": {
      "mse": "MSE: {0}",
      "_mse.comment": "the mean squared error, see https://en.wikipedia.org/wiki/Mean_squared_error",
      "rSquared": "R-squared: {0}",
      "_rSquared.comment": "the coefficient of determination, see https://en.wikipedia.org/wiki/Coefficient_of_determination",
      "meanPrediction": "Mean prediction {0}",
      "_meanPrediction.comment": "the average of all the predictions",
      "accuracy": "Accuracy: {0}",
      "_accuracy.comment": "computed accuracy of model on a subgroup, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "precision": "Precision: {0}",
      "_precision.comment": "computed precision of model, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "recall": "Recall: {0}",
      "_recall.comment": "computed recall of model, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "fpr": "FPR: {0}",
      "_fpr.comment": "False positive rate, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "fnr": "FNR: {0}",
      "_fnr.comment": "False negative rate, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers"
    },
    "GlobalOnlyChart": {
      "helperText": "Explore the top k important features that impact your overall model predictions. Use the slider to show descending feature importances."
    },
    "ExplanationSummary": {
      "whatDoExplanationsMean": "What do these explanations mean?",
      "clickHere": "Learn more",
      "shapTitle": "Shapley values",
      "shapDescription": "This explainer uses SHAP, which is a game theoretic approach to explaining models, where the importance of features sets is measured by \"hiding\" those features from the model through marginalization. Click the link below to learn more.",
      "limeTitle": "LIME (Local Interpretable Model-Agnostic Explanations)",
      "limeDescription": "This explainer uses LIME, which provides a linear approximation of the model. To get an explanation, we do the following: perturb the instance, get model predictions, and use these predictions as labels to learn a sparse linear model that is locally faithful. The weights of this linear model are used as 'feature importances'. Click the link below to learn more.",
      "mimicTitle": "Mimic (Global Surrogate Explanations)",
      "mimicDescription": "This explainer is based on the idea of training global surrogate models to mimic blackbox models. A global surrogate model is an intrinsically interpretable model that is trained to approximate the predictions of any black box model as accurately as possible. Feature importance values are model-based feature importance values of your underlying surrogate model (LightGBM, or Linear Regression, or Stochastic Gradient Descent, or Decision Tree)",
      "pfiTitle": "Permutation feature importance (PFI)",
      "pfiDescription": "This explainer randomly shuffles data one feature at a time for the entire dataset and calculates how much the performance metric of interest changes (default performance metrics: F1 for binary classification, F1 Score with micro average for multiclass classification and mean absolute error for regression). The larger the change, the more important that feature is. This explainer can only explain the overall behavior of the underlying model but does not explain individual predictions. Feature importance value of a feature represents the delta in the performance of the model by perturbing that particular feature."
    }
  },
  "ErrorAnalysis": {
    "defaultClassNames": "Class {0}",
    "_defaultClassNames.comment": " models that output classes have this as the default class names",
    "defaultFeatureNames": "Feature {0}",
    "_defaultFeatureNames.comment": "the default column names",
    "cohortInfo": "Cohort info",
    "correctPrediction": "Correct predictions",
    "incorrectPrediction": "Incorrect predictions",
    "whatIfDatapoints": "What-If datapoints",
    "allSelected": "All selected",
    "correctTotal": "Correct/Total",
    "dataExploration": "Dataset Exploration",
    "_dataExploration.comment": "Label for tab showing scatterplot of dataset and predictions",
    "instanceView": "Instance View",
    "_instanceView.comment": "Label for tab showing instances",
    "dataExplorerView": "Data explorer",
    "_dataExplorerView.comment": "Data explorer view",
    "errorCoverage": "Error coverage",
    "errorRate": "Error rate",
    "globalExplanationView": "Global explanation",
    "_globalExplanationView.comment": "Global explanation view",
    "localExplanationView": "Local explanation",
    "_localExplanationView.comment": "Local explanation view",
    "noFeature": "No feature",
    "globalImportance": "Global Importance",
    "_globalImportance.comment": "Label for tab showing bar chart of importance of features at a global level",
    "incorrectTotal": "Incorrect/Total",
    "treeMapDescription": "To retrain the tree map, select and save the features below.",
    "Cohort": {
      "cohort": "Cohort",
      "_cohort.comment": "a subset of the data is called a cohort",
      "defaultLabel": "All data"
    },
    "CohortInfo": {
      "cohortInformation": "Cohort information",
      "saveCohort": "Save as a new cohort"
    },
    "EditCohort": {
      "subText": "Learn about the selected cohort. Edit its cohort name. Delete this cohort.",
      "cohortName": "Cohort name"
    },
    "InspectionView": {
      "emptyError": "Please select datapoints in the incorrect and correct categories by clicking the checkboxes.",
      "selectedDatapoints": "Selected datapoints"
    },
    "InstanceView": {
      "selection": "{0} selected",
      "inspect": "Inspect"
    },
    "MainMenu": {
      "treeMap": "Tree map",
      "heatMap": "Heat map",
      "errorExplorerLabel": "Error explorer:",
      "errorExplorer": "Error explorer",
      "fullscreen": "Fullscreen",
      "whatIf": "What-If",
      "featureList": "Feature list",
      "shiftCohort": "Shift cohort",
      "saveCohort": "Save cohort",
      "cohortList": "Cohort list",
      "cohortInfo": "Cohort info",
      "explanation": "Explanation",
      "cohortSettings": "Cohort settings"
    },
    "MapShift": {
      "subText": "Do you want to restart your exploration with a different map?  Save it as a new cohort.  Otherwise, your chosen filters will be lost.",
      "title": "Map shift",
      "close": "Close",
      "move": "Move",
      "saveAs": "Save as a new cohort",
      "shift": "Shift",
      "cancel": "Cancel"
    },
    "MatrixArea": {
      "emptyText": "Select two features by using the dropdowns above.  You can cluster and filter your data along two dimensions.",
      "selectAll": "Select all",
      "clearAll": "Clear all"
    },
    "MatrixLegend": {
      "heatMapDescription": "With the grid map you can focus on specific filters and combine error rates.  Start with two dataset features to compare."
    },
    "Navigation": {
      "errorExplorer": "Error explorer",
      "dataExplorer": "Data explorer",
      "globalExplanation": "Global explanation",
      "localExplanation": "Local explanation",
      "localExplanationInspection": "Local explanation (Inspection)"
    },
    "SaveCohort": {
      "subText": "Save the current cohort to the cohort list. You can revisit the saved cohort via the cohort list.",
      "save": "Save",
      "saveTitle": "Save as a new cohort",
      "cancel": "Cancel",
      "close": "Close",
      "move": "Move",
      "cohortName": "Cohort name"
    },
    "TreeView": {
      "treeDescription": "To find nodes in the tree map with the most errors, look for higher values and fuller circles."
    },
    "WhatIfPanel": {
      "whatIfHeader": "What-If"
    }
  },
  "Fairness": {
    "loremIpsum": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "_loremIpsum.comment": "DO NOT TRANSLATE. This is placeholder text the user will NOT see",
    "defaultClassNames": "Class {0}",
    "_defaultClassNames.comment": "models that output classes have this as the default class names when name are not given by the user",
    "defaultFeatureNames": "Sensitive feature {0}",
    "_defaultFeatureNames.comment": "models that output classes have this as the default class names when name are not given by the user",
    "defaultSingleFeatureName": "Sensitive feature",
    "defaultCustomMetricName": "Custom metric {0}",
    "_defaultCustomMetricName.comment": "prepend in front of the numerical index of the custom metric from the list of custom metrics",
    "performanceTab": "Fairness in Performance",
    "opportunityTab": "Fairness in Opportunity",
    "modelComparisonTab": "Model comparison",
    "tableTab": "Detail view",
    "dataSpecifications": "Data statistics",
    "attributes": "Attributes",
    "singleAttributeCount": "1 sensitive feature",
    "attributesCount": "{0} sensitive features",
    "_attributesCount.comment": "formatted string of the number of attributes",
    "instanceCount": "{0} instances",
    "_instanceCount.comment": "formatted string of the number of instances",
    "close": "Close",
    "done": "Done",
    "calculating": "Calculating...",
    "performanceMetricLegacy": "Performance metric",
    "sensitiveFeatures": "01 Sensitive features",
    "performanceMetric": "02 Performance metrics",
    "fairnessMetric": "03 Fairness metrics",
    "errorOnInputs": "Error with input. Sensitive features must be categorical values at this time. Please map values to binned categories and retry.",
    "Performance": {
      "header": "How do you want to measure performance?",
      "modelMakes": "model makes",
      "modelsMake": "models make",
      "body": "Your data contains {0} labels and your {2} {1} predictions. Based on that information, we recommend the following metrics. Please select one metric from the list.",
      "_body.comment": "States whether labels are binary or continuous (0) and whether predictions are binary or continuous (1). (2) simply allows 'model(s) make' to be either singular or plural",
      "binaryClassifier": "binary classifier",
      "probabilisticRegressor": "probit regressor",
      "regressor": "regressor",
      "binary": "binary",
      "continuous": "continuous"
    },
    "Fairness": {
      "pickerHeader": "How do you want to measure fairness?",
      "header": "Fairness measured in terms of disparity",
      "bodyLegacy": "Fairness metrics quantify variation of your model's behavior across selected features. There are two of fairness metrics: more to come....",
      "body": "Fairness metrics quantify variation of your model's behavior across selected features. There are several kinds of fairness metrics that are based on a variety of performance metrics. They either capture the difference or ratio between the extreme values across the groups, or simply the worst value of any group."
    },
    "Header": {
      "title": "Fairness",
      "documentation": "Documentation"
    },
    "Footer": {
      "back": "Back",
      "next": "Next"
    },
    "Intro": {
      "welcome": "Welcome to the",
      "fairnessDashboard": "Fairness dashboard",
      "introBody": "The fairness dashboard enables you to assess tradeoffs between performance and fairness of your models",
      "explanatoryStep": "To set up the assessment, you need to specify a sensitive feature, a performance metric, and a fairness metric.",
      "getStarted": "Get started",
      "features": "Sensitive features",
      "featuresInfo": "Sensitive features are used to split your data into groups. Fairness of your model across these groups is measured by fairness metrics. Fairness metrics quantify how much your model's behavior varies across these groups.",
      "performance": "Performance metric",
      "performanceInfo": "Performance metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. The difference between the extreme values of the performance metric across the groups is reported as the disparity in performance.",
      "fairness": "Fairness metrics",
      "fairnessInfo": "Fairness metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. Fairness metrics may represent the difference or ratio between the extreme values of a performance metric, or simply the worst value of any group."
    },
    "ModelComparison": {
      "title": "Model comparison",
      "howToRead": "How to read this chart",
      "lower": "lower",
      "higher": "higher",
      "howToReadText": "This chart represents each of the {0} models as a selectable point. The x-axis represents {1}, with {2} being better. The y-axis represents disparity, with lower being better.",
      "_howToReadText.comment": "Instructions for reading a chart. The number of models in the chart (0), the metric shown of the x-axix (1), and orientation for interpreting the x-axis",
      "insightsLegacy": "Insights",
      "insights": "Key insights",
      "downloadReport": "Download report",
      "disparity": " The disparity ",
      "rangesFrom": " ranges from ",
      "to": " to ",
      "period": ". ",
      "introModalText": "Each model is a selectable point. Click or tap on a model for its full fairness assessment.",
      "helpModalText1": "The x-axis represents performance, with {0} being better.",
      "helpModalText2": "The y-axis represents fairness metric values, with {0} being better.",
      "insightsText2": "{0} ranges from {1} to {2}. {3} ranges from {4} to {5}.",
      "insightsText3": "The model with the best performance achieves {0} of {1} and {2} of {3}.",
      "insightsText4": "The model with the best fairness metric value achieves {0} of {1} and {2} of {3}.",
      "insightsText3v1FairnessMetric": "a disparity",
      "disparityInOutcomes": "Disparity in predictions",
      "disparityInPerformance": "Disparity in {0}",
      "howToMeasureDisparity": "How should disparity be measured?"
    },
    "Report": {
      "modelName": "Model {0}",
      "_modelName.comment": "The name of the model",
      "title": "Disparity in performance",
      "globalPerformanceText": "Is the overall {0}",
      "_globalPerformanceText.comment": "The title of the metric for which performance is being assessed",
      "performanceDisparityText": "Is the disparity in {0}",
      "_performanceDisparityText.comment": "The title of the metric for which performance is being assessed",
      "editConfiguration": "Edit configuration",
      "backToComparisonsLegacy": "Multimodel view",
      "backToComparisons": "Back to all models",
      "assessmentResults": "Assessment results for",
      "performanceChartHeaderBinaryClassification": "False positive and false negative rates",
      "performanceChartHeaderProbability": "Over- and underprediction",
      "performanceChartHeaderRegression": "Distribution of errors",
      "outcomesTitle": "Disparity in predictions",
      "expandSensitiveAttributes": "Expand sensitive attributes",
      "collapseSensitiveAttributes": "Collapse sensitive attributes",
      "minTag": "Min",
      "maxTag": "Max",
      "groupLabel": "Subgroup",
      "overallLabel": "Overall",
      "underestimationError": "Underprediction",
      "underpredictionExplanation": "(predicted = 0, true = 1)",
      "overpredictionExplanation": "(predicted = 1, true = 0)",
      "overestimationError": "Overprediction",
      "falseNegativeRate": "False negative rate",
      "falsePositiveRate": "False positive rate",
      "classificationOutcomesHowToRead": "The bar chart shows the selection rate in each group, meaning the fraction of points classified as 1.",
      "regressionOutcomesHowToRead": "Box plots show the distribution of predictions in each group. Individual data points are overlaid on top.",
      "classificationPerformanceHowToReadV2": "The bar chart shows the false negative and false positive rates in each group.",
      "classificationPerformanceHowToRead1": "The bar chart shows the distribution of errors in each group.",
      "classificationPerformanceHowToRead2": "Errors are split into overprediction errors (predicting 1 when the true label is 0), and underprediction errors (predicting 0 when the true label is 1).",
      "classificationPerformanceHowToRead3": "The reported rates are obtained by dividing the number of errors by the overall group size.",
      "probabilityPerformanceHowToRead1": "The bar chart shows mean absolute error in each group, split into overprediction and underprediction.",
      "probabilityPerformanceHowToRead2": "On each example, we measure the difference between the prediction and the label. If it is positive, we call it overprediction and if it is negative, we call it underprediction.",
      "probabilityPerformanceHowToRead3": "We report the sum of overprediction errors and the sum of underprediction errors divided by the overall group size.",
      "regressionPerformanceHowToRead": "Error is the difference between the prediction and the label. Box plots show the distribution of errors in each group. Individual data points are overlaid on top.",
      "distributionOfPredictions": "Distribution of predictions",
      "distributionOfErrors": "Distribution of errors",
      "tooltipPrediction": "Prediction: {0}",
      "_tooltipPrediction.comment": "Displays tooltip with the formatted numerical value of the prediction",
      "tooltipError": "Error: {0}",
      "_tooltipError.comment": "Displays tooltip with the formatted numerical value of the error",
      "chartChoiceDropdownHeader": "Charts"
    },
    "Feature": {
      "header": "Along which features would you like to evaluate your model's fairness?",
      "body": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's performance metric and predictions differ across these splits.",
      "learnMore": "Learn more",
      "summaryCategoricalCount": "This feature has {0} unique values",
      "_summaryCategoricalCount.comment": "Number of unique values of the feature",
      "summaryNumericCount": "This numeric feature ranges from {0} to {1}, and is grouped into {2} bins.",
      "_summaryNumericalCount.comment": "The numerical range (low and high values) of the feature, and number of bin groups within that range",
      "showCategories": "Show all",
      "hideCategories": "Collapse",
      "categoriesOverflow": "   and {0} additional categories",
      "_categoriesOverflow.comment": "??? NOT IN SRC - number of remaining additional categories",
      "editBinning": "Edit groups",
      "subgroups": "Subgroups"
    },
    "Metrics": {
      "accuracyScore": "Accuracy",
      "precisionScore": "Precision",
      "recallScore": "Recall",
      "zeroOneLoss": "Zero-one loss",
      "specificityScore": "Specificity score",
      "missRate": "Miss rate",
      "falloutRate": "Fallout rate",
      "maxError": "Max error",
      "meanAbsoluteError": "Mean absolute error",
      "meanSquaredError": " Mean squared error",
      "meanSquaredLogError": "Mean squared log error",
      "medianAbsoluteError": "Median absolute error",
      "average": "Average prediction",
      "selectionRate": "Selection rate",
      "overprediction": "Overprediction",
      "underprediction": "Underprediction",
      "falsePositiveRate": "False positive rate",
      "falseNegativeRate": "False negative rate",
      "r2_score": "R-squared score",
      "rms_error": "Root mean squared error",
      "auc": "Area under ROC curve",
      "balancedRootMeanSquaredError": "Balanced root mean squared error",
      "balancedAccuracy": "Balanced accuracy",
      "f1Score": "F1-score",
      "_f1Score.comment": "Data science terminology: https://en.wikipedia.org/wiki/F1_score",
      "logLoss": "Log loss",
      "_logLoss.comment": "Data science terminology",
      "accuracyDescription": "The fraction of data points classified correctly.",
      "precisionDescription": "The fraction of data points classified correctly among those classified as 1.",
      "recallDescription": "The fraction of data points classified correctly among those whose true label is 1. Alternative names: true positive rate, sensitivity.",
      "rmseDescription": "Square root of the average of squared errors.",
      "mseDescription": "The average of squared errors.",
      "meanAbsoluteErrorDescription": "The average of absolute values of errors. More robust to outliers than MSE.",
      "r2Description": "The fraction of variance in the labels explained by the model.",
      "f1ScoreDescription": "F1-score is the harmonic mean of precision and recall.",
      "aucDescription": "The quality of the predictions, viewed as scores, in separating positive examples from negative examples.",
      "balancedRMSEDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "balancedAccuracyDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "falsePositiveRateDescription": "The fraction of data points classified incorrectly among those whose true label is 0.",
      "falseNegativeRateDescription": "The fraction of data points classified incorrectly among those whose true label is 1.",
      "accuracyScoreDifference": "Accuracy score difference",
      "accuracyScoreDifferenceDescription": "The maximum difference in accuracy score between any two groups.",
      "accuracyScoreMin": "Minimum accuracy score",
      "accuracyScoreMinDescription": "The minimum accuracy score of all groups.",
      "accuracyScoreRatio": "Accuracy score ratio",
      "accuracyScoreRatioDescription": "The minimum ratio in accuracy score between any two groups.",
      "balancedAccuracyScoreMin": "Balanced accuracy score minimum",
      "balancedAccuracyScoreMinDescription": "The minimum of all groups' average recall on each class (0 and 1).",
      "demographicParityDifference": "Demographic parity difference",
      "demographicParityDifferenceDescription": "The maximum difference in selection rate, that is the fraction with predicted label 1, between any two groups.",
      "demographicParityRatio": "Demographic parity ratio",
      "demographicParityRatioDescription": "The minimum ratio of selection rates, that is the fraction with predicted label 1, between any two groups.",
      "equalizedOddsDifference": "Equalized odds difference",
      "equalizedOddsDifferenceDescription": "Either the maximum difference between true positive rates of any two groups or the maximum difference between false positive rates of any two groups, whichever is greater.",
      "equalizedOddsRatio": "Equalized odds ratio",
      "equalizedOddsRatioDescription": "Either the minimum ratio between true positive rates of any two groups or the minimum ratio between false positive rates of any two groups, whichever is smaller.",
      "trueNegativeRateDifference": "True negative rate difference",
      "errorRateDifference": "Error rate difference",
      "errorRateDifferenceDescription": "The maximum difference between the error rates of any two groups.",
      "errorRateRatio": "Error rate ratio",
      "errorRateRatioDescription": "The minimum ratio between the error rates of any two groups.",
      "f1ScoreMin": "Minimum F1-score",
      "f1ScoreMinDescription": "The minimum F1-score of all groups.",
      "falseNegativeRateDifference": "False negative rate difference",
      "falseNegativeRateDifferenceDescription": "The maximum difference between false negative rates of any two groups. Also sometimes referred to as miss rate difference.",
      "falseNegativeRateRatio": "False negative rate ratio",
      "falseNegativeRateRatioDescription": "The minimum ratio between false negative rates of any two groups. Also sometimes referred to as miss rate ratio.",
      "falsePositiveRateDifference": "False positive rate difference",
      "falsePositiveRateDifferenceDescription": "The maximum difference between false positive rates of any two groups. Also sometimes referred to as fall-out difference.",
      "falsePositiveRateRatio": "False positive rate ratio",
      "falsePositiveRateRatioDescription": "The minimum ratio between false positive rates of any two groups. Also sometimes referred to as fall-out ratio.",
      "logLossMax": "Maximum log loss",
      "logLossMaxDescription": "The maximum logistic loss of all groups.",
      "meanAbsoluteErrorMax": "Maximum mean absolute error",
      "meanAbsoluteErrorMaxDescription": "The maximum mean absolute error of all groups.",
      "meanSquaredErrorMax": "Maximum mean squared error",
      "meanSquaredErrorMaxDescription": "The maximum mean squared error of all groups.",
      "precisionScoreMin": "Minimum precision score",
      "precisionScoreMinDescription": "The minimum precision score of all groups.",
      "r2ScoreMin": "Minimum R2-score",
      "r2ScoreMinDescription": "The minimum R2-score of all groups.",
      "recallScoreMin": "Minimum recall score",
      "recallScoreMinDescription": "The minimum recall score of all groups.",
      "ROCAUCScoreMin": "Minimum ROC AUC score",
      "ROCAUCScoreMinDescription": "The minimum Area Under the Receiver Operating Characteristic Curve (ROC AUC) of all groups.",
      "trueNegativeRateDifferenceDescription": "The maximum difference between true negative rates of any two groups. Also sometimes referred to as specificity score difference.",
      "trueNegativeRateRatio": "True negative rate ratio",
      "trueNegativeRateRatioDescription": "The minimum ratio between true negative rates of any two groups. Also sometimes referred to as specificity score ratio.",
      "truePositiveRateDifference": "True positive rate difference",
      "truePositiveRateDifferenceDescription": "The maximum difference between true positive rates of any two groups. Also sometimes referred to as equal opportunity difference or recall score difference.",
      "truePositiveRateRatio": "True positive rate ratio",
      "truePositiveRateRatioDescription": "The minimum ratio between true positive rates of any two groups. Also sometimes referred to as equal opportunity ratio or recall score ratio.",
      "Groups": {
        "equalizedOdds": "Equalized odds",
        "classificationAccuracyAndErrorRate": "Accuracy / error rate",
        "regressionError": "Error",
        "selectionRate": "Demographic Parity / Selection rate",
        "trueNegativeRate": "True negative rate / specificity",
        "truePositiveRate": "True positive rate / recall / sensitivity",
        "falseNegativeRate": "False negative rate / miss rate",
        "falsePositiveRate": "False positive rate / fall-out",
        "precision": "Precision",
        "overUnderPrediction": "Over- and underprediction",
        "auc": "Area under ROC curve",
        "average": "Average prediction",
        "f1_score": "F1-score",
        "loss": "Loss",
        "r2_score": "R-squared score",
        "custom": "Custom metrics"
      }
    },
    "BinDialog": {
      "header": "Configure bins",
      "makeCategorical": "Treat as categorical",
      "save": "Save",
      "cancel": "Cancel",
      "numberOfBins": "Number of bins:",
      "categoryHeader": "Bin values:"
    },
    "DropdownHeaders": {
      "sensitiveFeature": "Sensitive feature",
      "performanceMetric": "Performance metric",
      "fairnessMetric": "Fairness metric"
    }
  }
}
