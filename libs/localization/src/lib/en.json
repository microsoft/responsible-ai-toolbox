{
  "Common": {
    "close": "Close",
    "tooltipButton": "Tooltip button",
    "identityFeature": "Identity feature",
    "infoTitle": "Additional information",
    "spinButton": "Spin",
    "editButton": "Edit",
    "decreaseValue": "Decrease value",
    "increaseValue": "Increase value",
    "decreaseValueByOne": "Decrease value by 1",
    "increaseValueByOne": "Increase value by 1",
    "loading": "Loading..."
  },
  "ChartContextMenu": {
    "hideData": "Hide data table",
    "viewData": "View data table",
    "viewInFullScreen": "View in full screen",
    "printChart": "Print chart",
    "downloadCSV": "Download CSV",
    "downloadPNG": "Download PNG image",
    "downloadJPEG": "Download JPEG image",
    "downloadPDF": "Download PDF document",
    "downloadSVG": "Download SVG vector image",
    "downloadXLS": "Download XLS"
  },
  "CausalAnalysis": {
    "AggregateView": {
      "binary": "Binary treatments: ",
      "binaryDescription": "On average in this sample, turning on this feature will cause the probability of class/label '{0}' to increase by X units.",
      "regressionDescription": "On average in this sample, turning on this feature will cause the predictions of the target to increase/decrease by X units.",
      "causalPoint": "Causal effect point",
      "confidenceLower": "Confidence interval (lower)",
      "confidenceUpper": "Confidence interval (upper)",
      "confoundingFeature": "A confounding feature is correlated with both the treatment and the outcome of interest. The confounder creates an extra correlation path from the treatment to the outcome on top of the direct causal effect. Unless confounding features are measured and included in the model, these extra correlations can bias estimates of the causal effect. The bias can be positive or negative, depending on the directions of correlations between omitted confounders, treatments, and outcomes.",
      "continuous": "Continuous treatments: ",
      "continuousDescription": "On average in this sample, increasing this feature by 1 unit will cause the probability of class/label '{0}' to increase by X units.",
      "continuousRegressionDescription": "On average in this sample, increasing this feature by 1 unit will cause the predictions of the target to increase/decrease by X units.",
      "description": "Causal analysis answers “what if” questions about how real world outcomes would have changed under different policy choices, such as a different pricing strategy for a product or an alternative treatment for a patient. Unlike model predictions that identify important correlation patterns, these tools help you identify the most important causal features that directly affect your outcome of interest. These models identify the causal effect of one feature (typically referred to as a “treatment”), holding other confounding features constant. For best results, make sure that the full dataset contains all available features that may correlate with the outcome as confounders.",
      "directAggregate": "Direct aggregate causal effect of each treatment with 95% confidence interval",
      "here": "here",
      "infoTitle": "Additional information on aggregated causal effects",
      "lasso": "A lasso (or logistic regression if y is binary) was fit to predict y from X[-i], and a lasso (or logistic regression if X[i] is categorical) was fit to predict X[i] from Χ[-i]. The causal effect can be viewed as the average correlation of the residuals/surprise variation of the two prediction tasks. Learn more about Double Machine Learning",
      "unconfounding": "What are confounding features?"
    },
    "IndividualView": {
      "currentOutcome": "Current outcome",
      "currentTreatment": "Current treatment value",
      "dataRequired": "This tab requires a datapoint be selected.",
      "selectedDatapoint": "Selected datapoint",
      "description": "Individual causal effects can inform personalized interventions, such as a targeted promotion to customers or an individualized treatment plan. How would an individual with a particular set of features respond to a change in a causal feature, or treatment?  The causal what-if tool calculates marginal changes in real-world outcomes for a particular individual if you change their level of a treatment. This analysis enables you to understand how real-world outcomes would have changed under different policy choices, such as a different pricing strategy for a product or an alternative treatment for a patient. Specify the treatment of interest and observe how the real-world outcome would change.",
      "directIndividual": "Direct individual causal effect of each treatment with 95% confidence interval",
      "index": "Datapoint index",
      "infoTitle": "Additional information on individual causal what-if",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "newOutcome": "New outcome",
      "selectTreatment": "Select treatment",
      "setNewTreatment": "Set new treatment value",
      "treatmentValue": "Treatment value min: {0} ({1}) , max: {2} ({3})"
    },
    "MainMenu": {
      "aggregate": "Aggregate causal effects",
      "cohortInfo": "Global cohorts are not currently supported for causal analysis. All causal analyses will be shown for all data.",
      "header": "causal analysis",
      "individual": "Individual causal what-if",
      "learnMore": "Learn more",
      "title": "Direct causal effect of each feature with 90% confidence interval",
      "treatment": "Treatment policy",
      "why": "Why is it important to include confounding features?"
    },
    "TreatmentPolicy": {
      "BarDescriptionBinary": "This graph shows the average gains of various treatment policies relative to the baseline of not providing the {0} treatment to anyone in your population.",
      "BarDescriptionContinuous": "This graph shows the average gains of various treatment policies relative to the baseline of not making any modifications of the current treatment values given to everyone in your population.",
      "BarTextBinary": "The first bar indicates average relative gains if the recommended global treatment policy described above were applied. The reported gains report the numerical change in treatment we used. The second bar indicates relative gains (or losses) from treating everyone by turning on binary treatments.",
      "BarTextContinuous": "The first bar indicates average relative gains if the recommended global treatment policy described above were applied. The reported gains reflect increasing/decreasing the treatment feature by 10% of the typical treatment size in the sample. The second bar indicates relative gains (or losses) from increasing/decreasing continuous treatments by 10%.",
      "Description": "These tools help build policies for future interventions. You can identify what parts of your sample experience the largest responses to a causal driver (i.e. treatment) or construct rules to define which future populations will have the largest treatment effects.",
      "Left": "{0} less than or equal to {1}",
      "Recommended": "Recommended treatment = {0}",
      "Right": "{0} greater than {1}",
      "SelectPolicy": "Set treatment feature",
      "Size": "Interpretable recommended global treatment policy for sample size (n) = {0}",
      "Table": "The table can be read by taking a row and then taking a column of that specific row.",
      "TableDescription": "This table shows a recommended treatment policy that can be applied to the current data sample or other populations. The table provides a simple rule to segment observations into data cohorts based on the features with the largest impact on whether the individual will respond to the selected treatment. The table also specifies number of datapoints in the current data sample assigned to each segment.",
      "alwaysTreat": "Always treat '{0}'",
      "averageGainBinary": "Average gains of setting the treatment {0} to its baseline value {1}.",
      "averageGainContinuous": "Average gains of alternative policies over no '{0}' treatment.",
      "header": "These tools help build policies for future interventions. You can identify what parts of your sample experience the largest responses to changes in causal features, or treatments, and construct rules to define which future populations should be targeted for particular interventions.",
      "infoTitle": "Additional information on treatment policy",
      "nSample": "n = {0}",
      "noData": "No data"
    }
  },
  "Core": {
    "ExpandableText": {
      "SeeLess": "See less",
      "SeeMore": "See more"
    },
    "NoData": {
      "Title": "No data"
    },
    "ComparisonTypes": {
      "lt": "{0} less than {1}",
      "lte": "{0} less than or equal to {1}",
      "gt": "{0} greater than {1}",
      "gte": "{0} greater than or equal to {1}",
      "eq": "{0} equal to {1}",
      "ne": "{0} not equal to {1}",
      "in": "{0} in the set of ({1})",
      "nin": "{0} not in the set of ({1})",
      "rg": "{0} in the range of ({1})",
      "nrg": "{0} in the range of ({1})",
      "Unknown": "unknown"
    },
    "ShiftCohort": {
      "apply": "Apply",
      "cancel": "Cancel",
      "title": "Switch Cohort",
      "subText": "Select a cohort from the cohort list. Apply the cohort to the dashboard.",
      "selectCohort": "Select a cohort",
      "cohortList": "Cohort list"
    },
    "PreBuiltCohort": {
      "featureNameNotFound": "Feature name not found in the dataset",
      "notACategoricalFeature": "Feature is not categorical"
    },
    "EmptyCohortDialog": {
      "title": "Dataset cohort contains no datapoints",
      "subText": "It seems your dataset cohort contains no datapoints, adjust your filters to include at least one datapoint in your dataset cohort before saving."
    }
  },
  "Counterfactuals": {
    "CurrentTreatment": "Current treatment",
    "EffectLowerBound": "CI lower",
    "EffectOfTreatment": "Effect of recommendation",
    "EffectUpperBound": "CI upper",
    "ErrorDialog": {
      "Close": "Close",
      "ErrorPrefix": "Error encountered: {0}",
      "PythonError": "Python error"
    },
    "RecommendedTreatment": "Recommended treatment",
    "WhatIf": {
      "setValue": "Set Value",
      "sortFeatures": "Sort feature columns by counterfactual feature importance",
      "percentCounterfactualLocalImportance": "Percentage of counterfactuals that varied the feature",
      "toggleToolTipBody": "Counterfactual examples generate local feature importances which indicate which top features were perturbed the most to achieve the desired class classification scenarios or desired range for regression scenarios.",
      "toggleToolTipHeader": "Ranked counterfactual features",
      "predictedClass": "Predicted class",
      "predictedValue": "Predicted value"
    },
    "Size": "Size",
    "loading": "Loading...",
    "counterfactualEx": "Counterfactual Ex {0}",
    "counterfactualName": "What-if counterfactual name",
    "createWhatIfCounterfactual": "Create what-if counterfactual",
    "createCounterfactual": "Counterfactual",
    "revertToBubbleChart": "View bubble chart",
    "createOwn": "Create your own counterfactual:",
    "currentClass": "Current class",
    "currentRange": "Current range",
    "decreaseByOne": "Decrease value by 1",
    "desiredClass": "Desired class for counterfactual(s)",
    "header": "What-If counterfactuals",
    "increaseByOne": "Increase value by 1",
    "individualTreatment": "Recommended individual treatment policy",
    "listChoose": "Use the spinner control to choose the top k samples you’d like to view.",
    "listDescription": "This list shows which datapoints in the current data sample have the largest causal response to the selected treatment, based on all features included in the estimated causal model. The left five columns report whether treatment is recommended for the observation, the current treatment, the estimated effect of treatment (effect of applying a treatment from a baseline of no treatment for binary treatments or increasing/decreasing the treatment feature by 10% of the typical treatment size in the sample: [dynamic: report the numerical change in treatment we used] ), and the lower and upper confidence intervals (CI) for this effect. The remaining columns show the current treatment status and other features of each observation.",
    "localImportanceDescription": "The top ranked features in Row {0} to perturb to achieve desired model prediction. Based on what-if analysis for prediction: {1}",
    "localImportanceSelectData": "Select a data point to view local importance chart",
    "largeLocalImportanceSelectData": "Select a bubble, followed by a data point to view local importance chart",
    "localImportanceFetchError": "There was an error while fetching the local importance data. Error details: {0} Please check the data used.",
    "BubbleChartFetchError": "There was an error while fetching the data. Error details: {0} Please check the data used.",
    "noData": "No data",
    "noFeatures": "No features available",
    "panelDescription": "Browse counterfactuals and create your own. Search features to see suggested values from a diverse set of counterfactual examples. Set suggested counterfactual feature values by clicking “Set Value” text under each counterfactual name. Name your counterfactual and save it.",
    "panelDescriptionWithoutSetValue": "Browse counterfactuals and create your own. Search features to see suggested values from a diverse set of counterfactual examples.",
    "whatIfPanelHeader": "What-if counterfactuals",
    "panelHeader": "Counterfactuals",
    "recommendedPolicy": "Recommended policy gain",
    "referenceDatapoint": "Reference datapoint: Row {0}",
    "saveAsNew": "Save as new datapoint",
    "saveDescription": "These counterfactuals are based on the model's prediction, and therefore may be capturing the same correlations that were picked up by the model. We do not advise users to make real-life decisions based on this tool. Use our toolkit for causal analysis for real-life decision-making scenarios.",
    "seePrediction": "See prediction deltas",
    "selectedDatapoint": "Selected datapoint",
    "showOnly": "Show only",
    "showTop": "Datapoints with the largest estimated causal responses to treatment feature: {0}",
    "whatifDescription": "What-if allows you to perturb features for any input and observe how the model's prediction changes. You can perturb features manually or specify the desired prediction (e.g., class label for a classifier) to see a list of closest data points to the original input that would lead to the desired prediction. Also known as prediction counterfactuals, you can use them for exploring the relationships learnt by the model; understanding important, necessary features for the model's predictions; or debug edge-cases for the model. To start, choose input points from the data table or scatter plot."
  },
  "ErrorAnalysis": {
    "Cohort": {
      "_cohort.comment": "a subset of the data is called a cohort",
      "cohort": "Cohort",
      "defaultLabel": "All data"
    },
    "CohortBaseAndFilters": {
      "globalCohortAndFilters": "Global cohort and filters",
      "globalCohort": "Global cohort",
      "errorExplorer": "Error explorer",
      "filters": "Filters"
    },
    "CohortInfo": {
      "baseCohortInstances": "Instances in global cohort",
      "basicInformation": "Basic Information",
      "cohortInformation": "Cohort information",
      "correct": "Correct",
      "filters": "filters",
      "incorrect": "Incorrect",
      "predictionPath": "Prediction path (filters)",
      "saveCohort": "Save as a new cohort",
      "selectedCohortInstances": "Instances in the selected cohort",
      "total": "Total"
    },
    "CohortList": {
      "coverage": "Coverage"
    },
    "EditCohort": {
      "cohortName": "Cohort name",
      "subText": "Learn about the selected cohort. Edit its cohort name. Delete this cohort."
    },
    "FeatureList": {
      "featureList": "Feature List",
      "apply": "Apply",
      "features": "Features",
      "importances": "Importances",
      "treeMapDescription": "To retrain the tree map, select and save the features below. The feature importances were calculated using mutual information with the error on the true labels.  Please use it as a guideline for training the tree map.",
      "staticTreeMapDescription": "View the features that were used to train the tree map. The feature importances were calculated using mutual information with the error on the true labels.",
      "searchResultMessage": "Results displayed out of {resultLength} for {searchValue}"
    },
    "TreeViewParameters": {
      "maximumDepth": "Maximum depth",
      "maximumDepthInfoText": "The maximum depth of the surrogate tree trained on errors",
      "maximumDepthTitle": "Additional information on maximum depth",
      "numLeaves": "Number of leaves",
      "numLeavesInfoText": "The number of leaves of the surrogate tree trained on errors",
      "numLeavesTitle": "Additional information on number of leaves",
      "minDataInLeaf": "Minimum number of samples in one leaf",
      "minDataInLeafInfoText": "The minimum number of data required to create one leaf",
      "minDataInLeafTitle": "Additional information on minimum number of samples in one leaf"
    },
    "TreeLegend": {
      "clearSelection": "Clear selection"
    },
    "FilterTooltip": {
      "correctNum": "Correct (#): ",
      "countNum": "Count (#): ",
      "errorSum": "Error sum: ",
      "incorrectNum": "Incorrect (#): "
    },
    "InspectionView": {
      "emptyError": "Please select datapoints in the incorrect and correct categories by clicking the checkboxes.",
      "selectedDatapoints": "Selected datapoints"
    },
    "InstanceView": {
      "inspect": "Inspect",
      "selection": "{0} selected"
    },
    "MainMenu": {
      "cohortInfo": "Cohort info",
      "cohortList": "Cohort list",
      "cohortSettings": "Cohort settings",
      "errorAnalysisLabel": "Error analysis",
      "errorExplorer": "Error explorer",
      "errorExplorerLabel": "Error explorer:",
      "explanation": "Explanation",
      "featureList": "Feature list",
      "fullscreen": "Fullscreen",
      "heatMap": "Heat map",
      "newCohort": "New cohort",
      "shiftCohort": "Switch cohort",
      "treeMap": "Tree map",
      "whatIf": "What-If"
    },
    "MapShift": {
      "cancel": "Cancel",
      "close": "Close",
      "move": "Move",
      "saveAs": "Save as a new cohort",
      "shift": "Shift",
      "heatTitle": "Leave tree view?",
      "heatSubText": "Are you sure you want to leave tree view? You can save your filters as a new cohort so you can access them in Cohort settings. Otherwise your filters will be lost.",
      "treeTitle": "Leave heatmap view?",
      "treeSubText": "Are you sure you want to leave heatmap view? You can save your filters as a new cohort so you can access them in Cohort settings. Otherwise your filters will be lost."
    },
    "MatrixArea": {
      "clearAll": "Clear all",
      "emptyText": "Select two features by using the dropdowns above.  You can cluster and filter your data along two dimensions.",
      "selectAll": "Select all"
    },
    "MatrixFilter": {
      "disabledWarning": "Error heatmap is disabled unless global cohort is switched to represent \"All data\" due to the heatmap being generated for the full dataset. Switch back to the full dataset to view the error heatmap."
    },
    "MatrixSummary": {
      "heatMapInfoTitle": "Additional information on heat map",
      "heatMapDescription": "With the heat map you can focus on specific intersectional feature filters and compute disaggregated error rates. Start with two dataset features to compare.",
      "heatMapStaticDescription": "With the heat map you can focus on specific intersectional feature filters and compute disaggregated error rates. Up to two features must be selected to create a heat map via SDK before viewing the dashboard."
    },
    "MatrixOptions": {
      "quantileBinningLabel": "Quantile binning",
      "binningThresholdLabel": "Binning threshold",
      "toggleOnLabel": "On",
      "toggleOffLabel": "Off",
      "quantileBinningInfoText": "Distribute values evenly across varying ranges of bins",
      "quantileBinningTitle": "Additional information on quantile binning",
      "binningThresholdInfoText": "Number of values required before binning",
      "binningThresholdTitle": "Additional information on binning threshold"
    },
    "Metrics": {
      "AccuracyScore": {
        "Name": "Accuracy score",
        "Info": "The accuracy score represents the ratio of correct to total instances in the data.",
        "Short": "Accuracy",
        "Title": "Additional information on accuracy score"
      },
      "ErrorRate": {
        "Name": "Error rate",
        "Info": "The error rate represents the percentage of instances in the node for which the system has failed.",
        "Short": "Error rate",
        "Title": "Additional information on error rate"
      },
      "F1Score": {
        "Name": "F1 score",
        "Info": "The F1 score is the harmonic mean of the precision and recall metrics.",
        "Short": "F1 score",
        "Title": "Additional information on F1 score"
      },
      "MeanAbsoluteError": {
        "Name": "Mean absolute error",
        "Info": "The mean absolute error is the average of the sum of the errors.",
        "Short": "Mean abs. error",
        "Title": "Additional information on mean absolute error"
      },
      "MeanSquaredError": {
        "Name": "Mean squared error",
        "Info": "The mean squared error is the average of the squares of the errors.",
        "Short": "Mean sq. error",
        "Title": "Additional information on mean squared error"
      },
      "Precision": {
        "Name": "Precision score",
        "Info": "The precision is the ratio of true positives over all predicted positives.",
        "Short": "Precision",
        "Title": "Additional information on precision"
      },
      "Recall": {
        "Name": "Recall score",
        "Info": "The recall is the ratio of true positives over all actual positives.",
        "Short": "Recall",
        "Title": "Additional information on recall"
      },
      "MacroPrecision": {
        "Name": "Macro averaged precision score",
        "Info": "The macro averaged precision is the ratio of true positives over all predicted positives computed independently per class and averaged.",
        "Short": "Macro precision",
        "Title": "Additional information on macro averaged precision"
      },
      "MicroPrecision": {
        "Name": "Micro averaged precision score",
        "Info": "The micro averaged precision is the ratio of true positives over all predicted positives aggregated for all classes.",
        "Short": "Micro precision",
        "Title": "Additional information on micro averaged precision"
      },
      "MacroRecall": {
        "Name": "Macro averaged recall score",
        "Info": "The macro averaged recall is the ratio of true positives over all actual positives computed independently per class and averaged.",
        "Short": "Macro recall",
        "Title": "Additional information on macro averaged recall"
      },
      "MicroRecall": {
        "Name": "Micro averaged recall score",
        "Info": "The micro averaged recall is the ratio of true positives over all actual positives aggregated for all classes.",
        "Short": "Micro recall",
        "Title": "Additional information on micro averaged recall"
      },
      "MacroF1Score": {
        "Name": "Macro averaged F1 score",
        "Info": "The macro averaged F1 score is the harmonic mean of the macro averaged precision and recall metrics.",
        "Short": "Macro F1 score",
        "Title": "Additional information on macro averaged F1 score"
      },
      "MicroF1Score": {
        "Name": "Micro averaged F1 score",
        "Info": "The micro averaged F1 score is the harmonic mean of the micro averaged precision and recall metrics.",
        "Short": "Micro F1 score",
        "Title": "Additional information on micro averaged F1 score"
      },
      "metricName": "Metric name",
      "metricValue": "Metric value"
    },
    "MetricSelector": {
      "selectorLabel": "Select metric",
      "feature1SelectorLabel": "Rows: Feature 1",
      "feature2SelectorLabel": "Columns: Feature 2"
    },
    "Navigation": {
      "cohortSaved": "New cohort is saved! See Cohort list under Cohort settings.",
      "dataExplorer": "Data explorer",
      "errorExplorer": "Error explorer",
      "globalExplanation": "Global explanation",
      "localExplanation": "Local explanation",
      "localExplanationInspection": "Local explanation (Inspection)"
    },
    "SaveCohort": {
      "cancel": "Cancel",
      "close": "Close",
      "cohortName": "Cohort name",
      "move": "Move",
      "save": "Save",
      "saveTitle": "Save as a new cohort",
      "subText": "Save the current cohort to the cohort settings. You can revisit the saved cohort via the cohort settings.",
      "defaultLabelCopy": "All data copy"
    },
    "TreeView": {
      "ariaLabel": "Interactive chart",
      "disabledArialLabel": "Disabled interactive chart",
      "treeMapInfoTitle": "Additional information on tree map",
      "treeDescription": "The tree visualization uses the mutual information between each feature and the error to best separate error instances from success instances hierarchically in the data. This simplifies the process of discovering and highlighting common failure patterns. To find important failure patterns, look for nodes with a stronger red color (i.e., high error rate) and a higher fill line (i.e., high error coverage). To edit the list of features being used in the tree, click on \"Feature list.\" Use the \"select metric\" dropdown menu to learn more about your error and success nodes' performance. Please note that this metric selection will not impact the way your error tree is generated.",
      "treeStaticDescription": "The tree visualization uses the mutual information between each feature and the error to best separate error instances from success instances hierarchically in the data. This simplifies the process of discovering and highlighting common failure patterns. To find important failure patterns, look for nodes with a stronger red color (i.e., high error rate) and a higher fill line (i.e., high error coverage). To view the list of features used in creating this error tree, click on \"Feature list.\" Use the \"select metric\" dropdown menu to learn more about your error and success nodes' performance. Please note that this metric selection will not impact the way your error tree is generated.",
      "disabledWarning": "Error treemap is disabled unless global cohort is switched to represent \"All data\" due to the treemap being generated for the full dataset. Switch back to the full dataset to view the error treemap."
    },
    "WhatIfPanel": {
      "whatIfHeader": "What-If"
    },
    "_dataExploration.comment": "Label for tab showing scatterplot of dataset and predictions",
    "_dataExplorerView.comment": "Data explorer view",
    "_defaultClassNames.comment": " models that output classes have this as the default class names",
    "_defaultFeatureNames.comment": "the default column names",
    "_globalExplanationView.comment": "Global explanation view",
    "_globalImportance.comment": "Label for tab showing bar chart of importance of features at a global level",
    "_instanceView.comment": "Label for tab showing instances",
    "_localExplanationView.comment": "Local explanation view",
    "allSelected": "All selected",
    "cells": "Cells",
    "cellsInfo": "The number of cells selected in the matrix filter.  When no cells selected, displays - (dash) and shows the global cohort information as if all cells are selected.",
    "cellsTitle": "Additional information on cells",
    "cohortInfo": "Cohort info",
    "correctPrediction": "Correct predictions",
    "correctTotal": "Correct/Total",
    "dataExploration": "Dataset Exploration",
    "dataExplorerView": "Data explorer",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Feature {0}",
    "errorCoverage": "Error coverage",
    "errorCoverageInfo": "The error coverage shows the percentage of errors that are concentrated in the selection out of all errors present in the dataset.",
    "errorCoverageTitle": "Additional information on error coverage",
    "globalExplanationView": "Global explanation",
    "globalImportance": "Global Importance",
    "incorrectPrediction": "Incorrect predictions",
    "incorrectTotal": "Incorrect/Total",
    "instanceView": "Instance View",
    "localExplanationView": "Local explanation",
    "noFeature": "No feature",
    "scaleWarning": "For scale reasons of the UI explanations are randomly downsampled",
    "whatIfDatapoints": "What-If datapoints"
  },
  "Fairness": {
    "ValidationErrors": {
      "missingPerformanceMetric": "No performance metric found.",
      "missingFairnessMetric": "No fairness metric found."
    },
    "BinDialog": {
      "cancel": "Cancel",
      "categoryHeader": "Bin values:",
      "header": "Configure bins",
      "makeCategorical": "Treat as categorical",
      "numberOfBins": "Number of bins:",
      "save": "Save"
    },
    "DropdownHeaders": {
      "errorMetric": "Enable error bars",
      "fairnessMetric": "Fairness metric",
      "performanceMetric": "Performance metric",
      "sensitiveFeature": "Sensitive feature"
    },
    "ErrorBounds": {
      "howToRead": "How to read these error bars",
      "introModalText1": "Error bars show the uncertainty in the metric values due to the size of the evaluation data set. We show 95% confidence intervals calculated using ",
      "wilsonScores": "Wilson scores",
      "introModalText2": " (for binary classification) or ",
      "normalApproximation": "normal approximation",
      "introModalText3": " (for other tasks)."
    },
    "Fairness": {
      "body": "Fairness metrics quantify variation of your model's behavior across selected features. There are several kinds of fairness metrics that are based on a variety of performance metrics. They either capture the difference or ratio between the extreme values across the groups, or simply the worst value of any group.",
      "bodyLegacy": "Fairness metrics quantify variation of your model's behavior across selected features. There are two of fairness metrics: more to come....",
      "header": "Fairness measured in terms of disparity",
      "pickerHeader": "How do you want to measure fairness?"
    },
    "Feature": {
      "_categoriesOverflow.comment": "??? NOT IN SRC - number of remaining additional categories",
      "_summaryCategoricalCount.comment": "Number of unique values of the feature",
      "_summaryNumericalCount.comment": "The numerical range (low and high values) of the feature, and number of bin groups within that range",
      "body": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's performance metric and predictions differ across these splits.",
      "categoriesOverflow": "   and {0} additional categories",
      "editBinning": "Edit groups",
      "header": "Along which features would you like to evaluate your model's fairness?",
      "hideCategories": "Collapse",
      "learnMore": "Learn more",
      "showCategories": "Show all",
      "subgroups": "Subgroups",
      "summaryCategoricalCount": "This feature has {0} unique values",
      "summaryNumericCount": "This numeric feature ranges from {0} to {1}, and is grouped into {2} bins."
    },
    "Footer": {
      "back": "Back",
      "next": "Next"
    },
    "Header": {
      "documentation": "Documentation",
      "title": "Fairness"
    },
    "Intro": {
      "explanatoryStep": "To set up the assessment, you need to specify a sensitive feature, a performance metric, and a fairness metric.",
      "fairness": "Fairness metrics",
      "fairnessDashboard": "Fairness dashboard",
      "fairnessInfo": "Fairness metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. Fairness metrics may represent the difference or ratio between the extreme values of a performance metric, or simply the worst value of any group.",
      "features": "Sensitive features",
      "featuresInfo": "Sensitive features are used to split your data into groups. Fairness of your model across these groups is measured by fairness metrics. Fairness metrics quantify how much your model's behavior varies across these groups.",
      "getStarted": "Get started",
      "introBody": "The fairness dashboard enables you to assess tradeoffs between performance and fairness of your models",
      "performance": "Performance metric",
      "performanceInfo": "Performance metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. The difference between the extreme values of the performance metric across the groups is reported as the disparity in performance.",
      "welcome": "Welcome to the"
    },
    "Metrics": {
      "Groups": {
        "auc": "Area under ROC curve",
        "average": "Average prediction",
        "classificationAccuracyAndErrorRate": "Accuracy / error rate",
        "custom": "Custom metrics",
        "equalizedOdds": "Equalized odds",
        "f1_score": "F1-score",
        "falseNegativeRate": "False negative rate / miss rate",
        "falsePositiveRate": "False positive rate / fall-out",
        "loss": "Loss",
        "overUnderPrediction": "Over- and underprediction",
        "precision": "Precision",
        "r2_score": "R-squared score",
        "regressionError": "Error",
        "selectionRate": "Demographic Parity / Selection rate",
        "trueNegativeRate": "True negative rate / specificity",
        "truePositiveRate": "True positive rate / recall / sensitivity"
      },
      "ROCAUCScoreMin": "Minimum ROC AUC score",
      "ROCAUCScoreMinDescription": "The minimum Area Under the Receiver Operating Characteristic Curve (ROC AUC) of all groups.",
      "_f1Score.comment": "Data science terminology: https://en.wikipedia.org/wiki/F1_score",
      "_logLoss.comment": "Data science terminology",
      "accuracyDescription": "The fraction of data points classified correctly.",
      "accuracyScore": "Accuracy",
      "accuracyScoreDifference": "Accuracy score difference",
      "accuracyScoreDifferenceDescription": "The maximum difference in accuracy score between any two groups.",
      "accuracyScoreMin": "Minimum accuracy score",
      "accuracyScoreMinDescription": "The minimum accuracy score of all groups.",
      "accuracyScoreRatio": "Accuracy score ratio",
      "accuracyScoreRatioDescription": "The minimum ratio in accuracy score between any two groups.",
      "auc": "Area under ROC curve",
      "aucDescription": "The quality of the predictions, viewed as scores, in separating positive examples from negative examples.",
      "average": "Average prediction",
      "balancedAccuracy": "Balanced accuracy",
      "balancedAccuracyDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "balancedAccuracyScoreMin": "Balanced accuracy score minimum",
      "balancedAccuracyScoreMinDescription": "The minimum of all groups' average recall on each class (0 and 1).",
      "balancedRMSEDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "balancedRootMeanSquaredError": "Balanced root mean squared error",
      "demographicParityDifference": "Demographic parity difference",
      "demographicParityDifferenceDescription": "The maximum difference in selection rate, that is the fraction with predicted label 1, between any two groups.",
      "demographicParityRatio": "Demographic parity ratio",
      "demographicParityRatioDescription": "The minimum ratio of selection rates, that is the fraction with predicted label 1, between any two groups.",
      "equalizedOddsDifference": "Equalized odds difference",
      "equalizedOddsDifferenceDescription": "Either the maximum difference between true positive rates of any two groups or the maximum difference between false positive rates of any two groups, whichever is greater.",
      "equalizedOddsRatio": "Equalized odds ratio",
      "equalizedOddsRatioDescription": "Either the minimum ratio between true positive rates of any two groups or the minimum ratio between false positive rates of any two groups, whichever is smaller.",
      "errorRateDifference": "Error rate difference",
      "errorRateDifferenceDescription": "The maximum difference between the error rates of any two groups.",
      "errorRateRatio": "Error rate ratio",
      "errorRateRatioDescription": "The minimum ratio between the error rates of any two groups.",
      "f1Score": "F1-score",
      "f1ScoreDescription": "F1-score is the harmonic mean of precision and recall.",
      "f1ScoreMin": "Minimum F1-score",
      "f1ScoreMinDescription": "The minimum F1-score of all groups.",
      "falloutRate": "Fallout rate",
      "falseNegativeRate": "False negative rate",
      "falseNegativeRateDescription": "The fraction of data points classified incorrectly among those whose true label is 1.",
      "falseNegativeRateDifference": "False negative rate difference",
      "falseNegativeRateDifferenceDescription": "The maximum difference between false negative rates of any two groups. Also sometimes referred to as miss rate difference.",
      "falseNegativeRateRatio": "False negative rate ratio",
      "falseNegativeRateRatioDescription": "The minimum ratio between false negative rates of any two groups. Also sometimes referred to as miss rate ratio.",
      "falsePositiveRate": "False positive rate",
      "falsePositiveRateDescription": "The fraction of data points classified incorrectly among those whose true label is 0.",
      "falsePositiveRateDifference": "False positive rate difference",
      "falsePositiveRateDifferenceDescription": "The maximum difference between false positive rates of any two groups. Also sometimes referred to as fall-out difference.",
      "falsePositiveRateRatio": "False positive rate ratio",
      "falsePositiveRateRatioDescription": "The minimum ratio between false positive rates of any two groups. Also sometimes referred to as fall-out ratio.",
      "logLoss": "Log loss",
      "logLossMax": "Maximum log loss",
      "logLossMaxDescription": "The maximum logistic loss of all groups.",
      "maxError": "Max error",
      "meanAbsoluteError": "Mean absolute error",
      "meanAbsoluteErrorDescription": "The average of absolute values of errors. More robust to outliers than MSE.",
      "meanAbsoluteErrorMax": "Maximum mean absolute error",
      "meanAbsoluteErrorMaxDescription": "The maximum mean absolute error of all groups.",
      "meanSquaredError": " Mean squared error",
      "meanSquaredErrorMax": "Maximum mean squared error",
      "meanSquaredErrorMaxDescription": "The maximum mean squared error of all groups.",
      "meanSquaredLogError": "Mean squared log error",
      "medianAbsoluteError": "Median absolute error",
      "missRate": "Miss rate",
      "mseDescription": "The average of squared errors.",
      "overprediction": "Overprediction",
      "precisionDescription": "The fraction of data points classified correctly among those classified as 1.",
      "precisionScore": "Precision",
      "precisionScoreMin": "Minimum precision score",
      "precisionScoreMinDescription": "The minimum precision score of all groups.",
      "r2Description": "The fraction of variance in the labels explained by the model.",
      "r2ScoreMin": "Minimum R2-score",
      "r2ScoreMinDescription": "The minimum R2-score of all groups.",
      "r2_score": "R-squared score",
      "recallDescription": "The fraction of data points classified correctly among those whose true label is 1. Alternative names: true positive rate, sensitivity.",
      "recallScore": "Recall",
      "recallScoreMin": "Minimum recall score",
      "recallScoreMinDescription": "The minimum recall score of all groups.",
      "rms_error": "Root mean squared error",
      "rmseDescription": "Square root of the average of squared errors.",
      "selectionRate": "Selection rate",
      "specificityScore": "Specificity score",
      "trueNegativeRateDifference": "True negative rate difference",
      "trueNegativeRateDifferenceDescription": "The maximum difference between true negative rates of any two groups. Also sometimes referred to as specificity score difference.",
      "trueNegativeRateRatio": "True negative rate ratio",
      "trueNegativeRateRatioDescription": "The minimum ratio between true negative rates of any two groups. Also sometimes referred to as specificity score ratio.",
      "truePositiveRateDifference": "True positive rate difference",
      "truePositiveRateDifferenceDescription": "The maximum difference between true positive rates of any two groups. Also sometimes referred to as equal opportunity difference or recall score difference.",
      "truePositiveRateRatio": "True positive rate ratio",
      "truePositiveRateRatioDescription": "The minimum ratio between true positive rates of any two groups. Also sometimes referred to as equal opportunity ratio or recall score ratio.",
      "underprediction": "Underprediction",
      "zeroOneLoss": "Zero-one loss"
    },
    "ModelComparison": {
      "_howToReadText.comment": "Instructions for reading a chart. The number of models in the chart (0), the metric shown of the x-axix (1), and orientation for interpreting the x-axis",
      "disparity": " The disparity ",
      "disparityInOutcomes": "Disparity in predictions",
      "disparityInPerformance": "Disparity in {0}",
      "downloadReport": "Download report",
      "helpModalText1": "The x-axis represents performance, with {0} being better.",
      "helpModalText2": "The y-axis represents fairness metric values, with {0} being better.",
      "higher": "higher",
      "howToMeasureDisparity": "How should disparity be measured?",
      "howToRead": "How to read this chart",
      "howToReadText": "This chart represents each of the {0} models as a selectable point. The x-axis represents {1}, with {2} being better. The y-axis represents disparity, with lower being better.",
      "insights": "Key insights",
      "insightsLegacy": "Insights",
      "insightsText2": "{0} ranges from {1} to {2}. {3} ranges from {4} to {5}.",
      "insightsText3": "The model with the best performance achieves {0} of {1} and {2} of {3}.",
      "insightsText3v1FairnessMetric": "a disparity",
      "insightsText4": "The model with the best fairness metric value achieves {0} of {1} and {2} of {3}.",
      "introModalText": "Each model is a selectable point. Click or tap on a model for its full fairness assessment.",
      "lower": "lower",
      "period": ". ",
      "rangesFrom": " ranges from ",
      "title": "Model comparison",
      "to": " to "
    },
    "Performance": {
      "_body.comment": "States whether labels are binary or continuous (0) and whether predictions are binary or continuous (1). (2) simply allows 'model(s) make' to be either singular or plural",
      "binary": "binary",
      "binaryClassifier": "binary classifier",
      "body": "Your data contains {0} labels and your {2} {1} predictions. Based on that information, we recommend the following metrics. Please select one metric from the list.",
      "continuous": "continuous",
      "header": "How do you want to measure performance?",
      "modelMakes": "model makes",
      "modelsMake": "models make",
      "probabilisticRegressor": "probit regressor",
      "regressor": "regressor"
    },
    "Report": {
      "_globalPerformanceText.comment": "The title of the metric for which performance is being assessed",
      "_modelName.comment": "The name of the model",
      "_performanceDisparityText.comment": "The title of the metric for which performance is being assessed",
      "_tooltipError.comment": "Displays tooltip with the formatted numerical value of the error",
      "_tooltipPrediction.comment": "Displays tooltip with the formatted numerical value of the prediction",
      "assessmentResults": "Assessment results for",
      "backToComparisons": "Back to all models",
      "backToComparisonsLegacy": "Multimodel view",
      "chartChoiceDropdownHeader": "Charts",
      "classificationOutcomesHowToRead": "The bar chart shows the selection rate in each group, meaning the fraction of points classified as 1.",
      "classificationPerformanceHowToRead1": "The bar chart shows the distribution of errors in each group.",
      "classificationPerformanceHowToRead2": "Errors are split into overprediction errors (predicting 1 when the true label is 0), and underprediction errors (predicting 0 when the true label is 1).",
      "classificationPerformanceHowToRead3": "The reported rates are obtained by dividing the number of errors by the overall group size.",
      "classificationPerformanceHowToReadV2": "The bar chart shows the false negative and false positive rates in each group.",
      "collapseSensitiveAttributes": "Collapse sensitive attributes",
      "distributionOfErrors": "Distribution of errors",
      "distributionOfPredictions": "Distribution of predictions",
      "editConfiguration": "Edit configuration",
      "expandSensitiveAttributes": "Expand sensitive attributes",
      "falseNegativeRate": "False negative rate",
      "falsePositiveRate": "False positive rate",
      "globalPerformanceText": "Is the overall {0}",
      "groupLabel": "Subgroup",
      "maxTag": "Max",
      "minTag": "Min",
      "modelName": "Model {0}",
      "outcomesTitle": "Disparity in predictions",
      "overallLabel": "Overall",
      "overestimationError": "Overprediction",
      "overpredictionExplanation": "(predicted = 1, true = 0)",
      "performanceChartHeaderBinaryClassification": "False positive and false negative rates",
      "performanceChartHeaderProbability": "Over- and underprediction",
      "performanceChartHeaderRegression": "Distribution of errors",
      "performanceDisparityText": "Is the disparity in {0}",
      "probabilityPerformanceHowToRead1": "The bar chart shows mean absolute error in each group, split into overprediction and underprediction.",
      "probabilityPerformanceHowToRead2": "On each example, we measure the difference between the prediction and the label. If it is positive, we call it overprediction and if it is negative, we call it underprediction.",
      "probabilityPerformanceHowToRead3": "We report the sum of overprediction errors and the sum of underprediction errors divided by the overall group size.",
      "regressionOutcomesHowToRead": "Box plots show the distribution of predictions in each group. Individual data points are overlaid on top.",
      "regressionPerformanceHowToRead": "Error is the difference between the prediction and the label. Box plots show the distribution of errors in each group. Individual data points are overlaid on top.",
      "title": "Disparity in performance",
      "tooltipError": "Error: {0}",
      "tooltipPrediction": "Prediction: {0}",
      "underestimationError": "Underprediction",
      "underpredictionExplanation": "(predicted = 0, true = 1)"
    },
    "_attributesCount.comment": "formatted string of the number of attributes",
    "_defaultClassNames.comment": "models that output classes have this as the default class names when name are not given by the user",
    "_defaultCustomMetricName.comment": "prepend in front of the numerical index of the custom metric from the list of custom metrics",
    "_defaultFeatureNames.comment": "models that output classes have this as the default class names when name are not given by the user",
    "_instanceCount.comment": "formatted string of the number of instances",
    "_loremIpsum.comment": "DO NOT TRANSLATE. This is placeholder text the user will NOT see",
    "attributes": "Attributes",
    "attributesCount": "{0} sensitive features",
    "calculating": "Calculating...",
    "close": "Close",
    "dataSpecifications": "Data statistics",
    "defaultClassNames": "Class {0}",
    "defaultCustomMetricName": "Custom metric {0}",
    "defaultFeatureNames": "Sensitive feature {0}",
    "defaultSingleFeatureName": "Sensitive feature",
    "done": "Done",
    "errorOnInputs": "Error with input. Sensitive features must be categorical values at this time. Please map values to binned categories and retry.",
    "fairnessMetric": "03 Fairness metrics",
    "instanceCount": "{0} instances",
    "loremIpsum": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "modelComparisonTab": "Model comparison",
    "opportunityTab": "Fairness in Opportunity",
    "performanceMetric": "02 Performance metrics",
    "performanceMetricLegacy": "Performance metric",
    "performanceTab": "Fairness in Performance",
    "sensitiveFeatures": "01 Sensitive features",
    "singleAttributeCount": "1 sensitive feature",
    "tableTab": "Detail view"
  },
  "Interpret": {
    "AggregateImportance": {
      "_featureLabel.comment": "Prefix to the feature name",
      "_high.comment": "label for the high end of the color bar",
      "_importanceLabel.comment": "prefix to the feature importance",
      "_low.comment": "The low end of the color bar",
      "_noColor.comment": "label for dropdown, do not apply any grouping",
      "_predictedClass.comment": "Label for dropdown option, group data by predicted class from model",
      "_predictedClassTooltip.comment": "prefixed in front of the output class names predicted by the model",
      "_predictedOutputTooltip.comment": "prefixed in front of the output in a regression model (numeric, no classes)",
      "_predictedValue.comment": "Label for dropdown option, group data by the predicted value from the model (numeric values)",
      "_scaledFeatureValue.comment": "The chart shows all data in a color scale (normalized to 0 - 1). This is the label for the color bar",
      "_tooManyRows.comment": "error message if the dataset is too large to visualize",
      "_topKFeatures.comment": "Label for slider to show only the top (k) most important features, where the slider is used to set the value of k",
      "_trueClass.comment": "label for dropdown, group data by true class",
      "_trueClassTooltip.comment": "prefixed in front of the true class labels",
      "_trueOutputTooltip.comment": "prefixed in front of the true value in a regression model (numeric, no classes)",
      "_trueValue.comment": "label for dropdown option, group data by true value (numeric)",
      "_valueLabel.comment": "prefix to the feature value",
      "featureLabel": "Feature: {0}",
      "high": "High",
      "importanceLabel": "Importance: {0}",
      "low": "Low",
      "noColor": "None",
      "predictedClass": "Predicted class",
      "predictedClassTooltip": "Predicted class: {0}",
      "predictedOutputTooltip": "Predicted output: {0}",
      "predictedValue": "Predicted value",
      "scaledFeatureValue": "Scaled feature value",
      "tooManyRows": "The provided dataset is larger than this chart can support",
      "topKFeatures": "Top K Features:",
      "topKInfo": "How top k is calculated",
      "trueClass": "True class",
      "trueClassTooltip": "True class: {0}",
      "trueOutputTooltip": "True output: {0}",
      "trueValue": "True value",
      "valueLabel": "Feature value: {0}"
    },
    "AxisConfigDialog": {
      "TreatAsCategorical": "Treat as categorical",
      "_TreatAsCategorical.comment": "a checkbox label to treat integers as categories instead of as numbers",
      "_binLabel.comment": "group all values into a fixed number of groups (bins)",
      "_ditherLabel.comment": "checkbox label for if small random changes should be added to numbers to more easily see large clusters",
      "_groupByCohort.comment": "if user selects to group by cohort, no further parameters to set, just show a message to fill space",
      "_numberOfBins.comment": "the number of groups (bins) to place all values into",
      "_select.comment": "label above dropdown to promp user to pick a feature",
      "_selectClass.comment": "label for dropdown listing all classes",
      "_selectFeature.comment": "dropdown label to select feature (column) for charting",
      "_selectFilter.comment": "label on dropdown to pick value for charting",
      "apply": "Apply",
      "binLabel": "Apply binning to data",
      "countHelperText": "A histogram of the number of points",
      "ditherLabel": "Should dither",
      "groupByCohort": "Group by cohort",
      "logarithmicScaling": "Enable logarithmic scaling",
      "numOfBins": "Number of bins",
      "selectClass": "Select class",
      "selectFeature": "Select feature",
      "selectFilter": "Select your axis value"
    },
    "BarChart": {
      "_absoluteGlobal.comment": "sorting option, sort by the absolute value of the importance of all datapoints",
      "_absoluteLocal.comment": "sorting option, sort by the absolute value of the importance for the single selected point",
      "_calculatingExplanation.comment": "loading message",
      "_classLabel.comment": "Prefix for class",
      "_noData.comment": "Error message for no applicable data",
      "_sortBy.comment": "prompt for setting how values are sorted",
      "absoluteGlobal": "Absolute global",
      "absoluteLocal": "Absolute local",
      "calculatingExplanation": "Calculating explanation",
      "classLabel": "Class: {0}",
      "noData": "No data",
      "sortBy": "Sort by"
    },
    "Charts": {
      "_countTooltipPrefix.comment": "on hover, show the prefix followed by the total number of points",
      "_featurePrefix.comment": "label shown before feature name in tooltip",
      "_importance.comment": "label shown before importance value in tooltip",
      "_numberOfDatapoints.comment": "some charts will always show the count of the number of rows in a group, this is the axis label on the chart",
      "_rowIndex.comment": "the index of a row in a dataset",
      "_xValue.comment": "label for x value button on chart",
      "_yValue.comment": "label for y value button on chart",
      "cohort": "Cohort",
      "count": "Count",
      "countTooltipPrefix": "Count: {0}",
      "featureImportance": "Feature importance",
      "featurePrefix": "Feature",
      "howToRead": "How to read this chart",
      "importancePrefix": "Importance",
      "numberOfDatapoints": "Number of datapoints",
      "rowIndex": "Row index",
      "absoluteIndex": "Absolute index",
      "xValue": "X-value",
      "yValue": "Y-value"
    },
    "Cohort": {
      "_cohort.comment": "a subset of the data is called a cohort",
      "cohort": "Cohort",
      "defaultLabel": "All data",
      "temporaryCohort": "Temporary cohort"
    },
    "CohortBanner": {
      "_addCohort.comment": "button text to create a new cohort",
      "_binaryClassifier.comment": "a model that predicts true or false is a binary classifier, this is the label",
      "_copy.comment": "suffix attached to name of cohort created by copying other cohort, by default.",
      "_dataStatistics.comment": "label for section containing statistics about the dataset",
      "_datapoints.comment": "formatted string of the number of datapoints in the dataset",
      "_datasetCohorts.comment": "a subset of the original data, defined by filtering the data. This is the label for presenting all cohorts the user created",
      "_duplicateCohort.comment": "button text to copy an existing cohort",
      "_editCohort.comment": "button text to edit the filters defining an existing cohort",
      "_features.comment": "formatted string of the number of features (columns) in a dataset",
      "_filters.comment": "the number of filters that define a cohort",
      "_multiclassClassifier.comment": "models that output a category, with more than two categories",
      "_regressor.comment": "a class of models that output a numeric score, name is derived from statistical regression",
      "addCohort": "New cohort",
      "binaryClassifier": "Binary classifier",
      "copy": " copy",
      "dataStatistics": "Data statistics",
      "datapoints": "{0} datapoints",
      "datasetCohorts": "Dataset cohorts",
      "details": "Details",
      "duplicateCohort": "Duplicate",
      "edit": "Edit",
      "editCohort": "Edit cohort",
      "features": "{0} features",
      "filters": "{0} filters",
      "multiclassClassifier": "Multiclass classifier",
      "name": "Name",
      "regressor": "Regressor"
    },
    "CohortEditor": {
      "columns": {
        "index": "Index",
        "dataset": "Dataset",
        "predictedY": "Predicted Y",
        "trueY": "True Y",
        "classificationOutcome": "Classification outcome",
        "regressionError": "Error"
      },
      "TreatAsCategorical": "Treat as categorical",
      "_TreatAsCategorical.comment": "a checkbox label to treat integers as categories instead of as numbers",
      "_addFilter.comment": "button text to add the current settings as a new filter",
      "_addedFilters.comment": "header above the list of filters that have been saved",
      "_cohortNameError.comment": "error message if required name is missing",
      "_cohortNameLabel.comment": "label for text filed where user adds name of a cohort (subset)",
      "_cohortNamePlaceholder.comment": "placeholder for cohort name",
      "_defaultFilterState.comment": "placeholder text prompting user to start making a filter",
      "_noAddedFilters.comment": "placeholder text when no filters are included",
      "_placeholderName.comment": "starting name for a new cohort",
      "_selectFilter.comment": "prompt to select an attribute to filter on",
      "addFilter": "Add filter",
      "addedFilters": "Filters",
      "cancel": "Cancel",
      "cancelExistingCohort": "Are you sure you want to cancel editing cohort and go back?",
      "cancelNewCohort": "Are you sure you want to cancel creating a new cohort and go back?",
      "cancelNo": "No",
      "cancelTitle": "Cancel cohort",
      "cancelYes": "Yes",
      "cohortNameError": "Missing cohort name",
      "cohortNameDupError": "Cohort with same name already exists",
      "cohortNameLabel": "Dataset cohort name",
      "cohortNamePlaceholder": "Name your cohort",
      "clearAllFilters": "Clear all filters",
      "defaultFilterState": "Select a filter to add parameters to your dataset cohort. Filters from your current viewing cohort are pre-populated.",
      "delete": "Delete",
      "invalidValueError": "Value should be between {0} and {1}",
      "minimumGreaterThanMaximum": "Minimum value inputs must be less than maximum value inputs.",
      "noAddedFilters": "No filters",
      "placeholderName": "Cohort {0}",
      "save": "Save",
      "saveAndSwitch": "Save and switch",
      "selectFilter": "Select filter",
      "noFiltersApplied": "No filters applied",
      "filterAdded": "Filter added"
    },
    "Columns": {
      "_classificationOutcome.comment": "Whether the prediction from the model matched the true value",
      "_none.comment": "option to not have data on this axis, instead just counts number of points",
      "_regressionError.comment": "true value minus predicted value is regression error",
      "classificationOutcome": "Classification outcome",
      "dataset": "Dataset",
      "error": "Error",
      "falseNegative": "False negative",
      "falsePositive": "False positive",
      "none": "Count",
      "predictedProbabilities": "Prediction probabilities",
      "predictedLabels": "Predicted labels",
      "trueLabels": "True labels",
      "regressionError": "Regression error",
      "trueNegative": "True negative",
      "truePositive": "True positive",
      "correctlyClassified": "Correctly classified",
      "misclassified": "Misclassified"
    },
    "CrossClass": {
      "_enumeratedClassInfo.comment": "explains the weights for selecting a single class",
      "_info.comment": "tooltip on hover",
      "_label.comment": "label for dropdown allowing user to select how importance across multiple output classes is aggregated",
      "_overview.comment": "explains absolute value weights",
      "_predictedClassInfo.comment": "explains predicted class weight",
      "absoluteValInfo": "Average of absolute value: Shows the sum of the feature's importance across all possible classes, divided by number of classes",
      "close": "Close",
      "crossClassWeights": "Cross class weights",
      "enumeratedClassInfo": "Enumerated class names: Shows only the specified class's feature importance values across all data points.",
      "info": "Information on cross-class calculation",
      "label": "Cross-class weighting:",
      "overviewInfo": "Multiclass models generate an independent feature importance vector for each class. Each class's feature importance vector demonstrates which features made a class more likely or less likely. You can select how the weights of the per-class feature importance vectors are summarized into a single value:",
      "predictedClassInfo": "Predicted class: Shows the feature importance value for a given point's predicted class"
    },
    "DatasetExplorer": {
      "_aggregatePlots.comment": "configuration option to view plots that sum or average points, opposed to sowing each point on its own",
      "_chartType.comment": "label for dropdown to select chart format",
      "_colorValue.comment": "label on button to set how data is mapped to color in chart",
      "_helperText.comment": "paragraph summarizing the view on this page and what actions the user can take",
      "_individualDatapoints.comment": "configuration option to view chart with each individual point, opposed to chart summing/averaging points",
      "_missingPArameters.comment": "Show a message if the required dataset parameter is not provided",
      "_noColor.comment": "placeholder text when no color axis picked",
      "aggregatePlots": "Aggregate plots",
      "chartType": "Chart type",
      "colorValue": "Color value",
      "infoTitle": "Additional information on data analysis chart view",
      "helperText": "Create dataset cohorts to analyze dataset statistics along filters such as predicted outcome, dataset features and error groups. Learn about the over/under presentation in your dataset.",
      "individualDatapoints": "Individual datapoints",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "noColor": "None",
      "datasetCohortDropdown": "Dataset cohort dropdown"
    },
    "DependencePlot": {
      "_featureImportanceOf.comment": "axis label on chart showing the importance of a selected feature (column)",
      "_placeholder.comment": "placeholder text explaining how to activate this chart, by clicking on a neighboring chart.",
      "featureImportanceOf": "Feature importance of",
      "placeholder": "Select a feature to show its dependence plot"
    },
    "ExplanationScatter": {
      "_class.comment": "label for predicted class",
      "_colorValue.comment": "label for selecting color value",
      "_count.comment": "the default axis is the count of items",
      "_dataLabel.comment": "prepend in front of column names",
      "_importanceLabel.comment": "prepend in front of feature importance of column name",
      "_index.comment": "the index value (an integer of the row number)",
      "_predictedY.comment": "predicted output",
      "_probabilityLabel.comment": "Probability prefix for all classes in a multiclass problem",
      "_trueY.comment": "The true value to be predicted",
      "_xValue.comment": "label for x value dropdown",
      "_yValue.comment": "label for y value dropdown",
      "class": "class: ",
      "colorValue": "Color:",
      "count": "Count",
      "dataGroupLabel": "Data",
      "dataLabel": "Data : {0}",
      "importanceLabel": "Importance : {0}",
      "index": "Index",
      "output": "Output",
      "predictedY": "Predicted Y",
      "probabilityLabel": "Probability : {0}",
      "trueY": "True Y",
      "xValue": "X value:",
      "yValue": "Y value:"
    },
    "ExplanationSummary": {
      "clickHere": "Learn more",
      "limeDescription": "This explainer uses LIME, which provides a linear approximation of the model. To get an explanation, we do the following: perturb the instance, get model predictions, and use these predictions as labels to learn a sparse linear model that is locally faithful. The weights of this linear model are used as 'feature importances'. Click the link below to learn more.",
      "limeTitle": "LIME (Local Interpretable Model-Agnostic Explanations)",
      "mimicDescription": "This explainer is based on the idea of training global surrogate models to mimic blackbox models. A global surrogate model is an intrinsically interpretable model that is trained to approximate the predictions of any black box model as accurately as possible. Feature importance values are model-based feature importance values of your underlying surrogate model (LightGBM, or Linear Regression, or Stochastic Gradient Descent, or Decision Tree).",
      "mimicTitle": "Mimic (Global Surrogate Explanations)",
      "pfiDescription": "This explainer randomly shuffles data one feature at a time for the entire dataset and calculates how much the performance metric of interest changes (default performance metrics: F1 for binary classification, F1 Score with micro average for multiclass classification and mean absolute error for regression). The larger the change, the more important that feature is. This explainer can only explain the overall behavior of the underlying model but does not explain individual predictions. Feature importance value of a feature represents the delta in the performance of the model by perturbing that particular feature.",
      "pfiTitle": "Permutation feature importance (PFI)",
      "shapDescription": "This explainer uses SHAP, which is a game theoretic approach to explaining models, where the importance of features sets is measured by \"hiding\" those features from the model through marginalization. Click the link below to learn more.",
      "shapTitle": "Shapley values",
      "whatDoFeatureMean": "What do these feature importance values mean?"
    },
    "FeatureImportanceWrapper": {
      "_barText.comment": "a bar plot ",
      "_beehiveText.comment": "A swarm plot (its like a scatter plot with categorical x axis with dithering, see examples https://seaborn.pydata.org/generated/seaborn.swarmplot.html)",
      "_boxText.comment": "a box plot https://en.wikipedia.org/wiki/Box_plot",
      "_chartType.comment": "label for dropdown to select chart format",
      "_globalImportanceExplanation.comment": "explains how global feature importance is calculated ",
      "_multiclassImportanceAddendum.comment": "explains how global importance is calculated for each class in a multiclass case.",
      "_violinText.comment": "a violin plot https://en.wikipedia.org/wiki/Violin_plot",
      "barText": "Bar",
      "beehiveText": "Swarm",
      "boxText": "Box",
      "chartType": "Chart type:",
      "globalImportanceExplanation": "Global feature importance is calculated by averaging the absolute value of the feature importance of all points (L1 normalization). ",
      "multiclassImportanceAddendum": "All points are included in calculating a feature's importance for all classes, no differential weighting is used. So a feature that has large negative importance for many points predicted to not be of 'Class A' will greatly increase that feature's 'Class A'  importance.",
      "violinText": "Violin"
    },
    "FilterOperations": {
      "_includes.comment": "tooltip label for a filter with included values",
      "_overflowFilterArgs.comment": "first placeholder is the first one or two items in a long list, the second placeholder is the count of remaining items",
      "equals": " equal to {0}",
      "excludes": " excludes {0} ",
      "greaterThan": " greater than {0}",
      "greaterThanEquals": " greater than or equal to {0}",
      "inTheRangeOf": "[ {0} ]",
      "includes": " includes {0} ",
      "lessThan": " less than {0}",
      "lessThanEquals": " less than or equal to {0}",
      "overflowFilterArgs": "{0} and {1} others"
    },
    "Filters": {
      "_categoricalIncludeValues.comment": "filter to selected categories",
      "_equalComparison.comment": "filter for rows that are exactly equal",
      "_greaterThanComparison.comment": "filter for rows that are greater than a value",
      "_greaterThanEqualToComparison.comment": "filter for rows that are greater than or equal to a value",
      "_inTheRangeOf.comment": "filter for rows that are between two values",
      "_lessThanComparison.comment": "filter for rows that are less than a value",
      "_lessThanEqualToComparison.comment": "filter for rows that are less than or equal to a value",
      "_numericValue.comment": "the value to compare to in greater/less than or equal to filter",
      "_numericalComparison.comment": "label for dropdown containing [greater than, less than, equal to]",
      "_uniqueValues.comment": "the number of unique values for a selected categorical filter",
      "categoricalIncludeValues": "Included values:",
      "equalComparison": "Equal to",
      "greaterThanComparison": "Greater than",
      "greaterThanEqualToComparison": "Greater than or equal to",
      "inTheRangeOf": "In the range of",
      "lessThanComparison": "Less than",
      "lessThanEqualToComparison": "Less than or equal to",
      "max": "Max: {0}",
      "maximum": "Maximum",
      "min": "Min: {0}",
      "minimum": "Minimum",
      "numericValue": "Value",
      "numericalComparison": "Operation",
      "uniqueValues": "# of unique values: {0}"
    },
    "GlobalOnlyChart": {
      "helperText": "Explore the top k important features that impact your overall model predictions. Use the slider to show descending feature importances."
    },
    "GlobalTab": {
      "_aggregateFeatureImportance.comment": "graph label for aggregated (summed and averaged) feature importances",
      "_datasetCohortSelector.comment": "label for selecting single cohort",
      "_datasetCohorts.comment": "label for dropdown allowing users to select what cohorts to view",
      "_helperText.comment": "paragraph summarizing the view on this page and available actions",
      "_legendHelperText.comment": "explanatory text on what actions can be done on a list of cohorts",
      "_missingParameters.comment": "Show a message if the required feature importance parameter is not provided",
      "_sortBy.comment": "prompt for setting how values are sorted",
      "_topAtoB.comment": "label on a slider, will tell user the index of the features they are currently seeing, like Top 5-10 features",
      "_viewDependencePlotFor.comment": "label for dropdown to select feature to be shown in a dependence plot (a kind of graph)",
      "_weightOptions.comment": "Weight how importance values are averaged https://en.wikipedia.org/wiki/Weighted_arithmetic_mean",
      "absoluteValues": "View as absolute values",
      "aggregateFeatureImportance": "Aggregate feature importance",
      "collapsedHelperText": "Explore the top k important features that impact your overall model predictions.",
      "datapoint": "Datapoint",
      "datasetCohortSelector": "Select a dataset cohort",
      "datasetCohorts": "Dataset cohorts",
      "datasetRequired": "Dependence plots require the evaluation dataset and local feature importance array.",
      "dependencePlotFeatureSelectPlaceholder": "Select feature",
      "dependencePlotHelperText": "This dependence plot shows the relationship of the values of a feature to its corresponding feature importance values.",
      "dependencePlotTitle": "Dependence plots",
      "helperText": "Explore the top-k important features that impact your overall model predictions (a.k.a. global explanation). Use the slider to show descending feature importances. All cohorts’ feature importances are shown side by side and can be toggled off by selecting the cohort in the legend. Click on any of the features in the graph to see a density plot below of how values of the selected feature affect prediction.",
      "infoTitle": "Additional information on aggregate feature importance",
      "legendHelpText": "Toggle cohorts on and off in the plot by clicking on the legend items.",
      "missingParameters": "This tab requires the local feature importance parameter be supplied.",
      "sortByCohort": "Sort by cohort",
      "sortBy": "Sort by datapoint",
      "topAtoB": "Top {0} features by their importance",
      "viewDependencePlotFor": "View dependence plot for:",
      "weightOptions": "Class importance weights",
      "weightOptionsDropdown": "Class importance weights dropdown"
    },
    "IcePlot": {
      "_errorPrefix.comment": "prefix in front of external error",
      "_featurePicker.comment": "feature dropdown label",
      "_integerError.comment": "error message if non-integer values typed by user",
      "_loadingMessage.comment": "loading message",
      "_maximumInputLabel.comment": "set maximum bounds label",
      "_minimumInputLabel.comment": "Set minimum bounds label",
      "_noModelError.comment": "error message for no model present",
      "_numericError.comment": "error message if non-numeric characters typed",
      "_predictedProbability.comment": "predicted probability label for y-axis",
      "_prediction.comment": "Prediction label for y-axis",
      "_predictionLabel.comment": "prediction hover prefix",
      "_probabilityLabel.comment": "probability hover prefix",
      "_stepInputLabel.comment": "number of samples to include between minimum and maximum (integer)",
      "_submitPrompt.comment": "prompt to user giving instructions to enter a numeric range",
      "_topLevelErrorMessage.comment": "error message for any parameter issue",
      "close": "Close",
      "errorPrefix": "Error encountered: {0}",
      "featurePickerLabel": "Feature:",
      "integerError": "Must be an integer",
      "loadingMessage": "Loading data...",
      "maximumInputLabel": "Maximum:",
      "minimumInputLabel": "Minimum:",
      "noModelError": "Please provide an operationalized model to explore predictions in ICE plots.",
      "numericError": "Must be numeric",
      "predictedProbability": "Predicted probability",
      "prediction": "Prediction",
      "predictionLabel": "Prediction: {0}",
      "probabilityLabel": "Probability: {0}",
      "pythonError": "Python error",
      "stepInputLabel": "Steps:",
      "submitPrompt": "Submit a range to view an ICE plot",
      "topLevelErrorMessage": "Error in parameter"
    },
    "ModelPerformance": {
      "_cohortPickerLabel.comment": "label for single-select dropdown to pick a cohort to view",
      "_helperText.comment": "explains the view on this page and what actions can be taken",
      "_missingPArameters.comment": "Show a message if the required prediction array not provided",
      "_missingTrueY.comment": "Show message if true Y values are not provided, since statistics require the true value",
      "_modelStatistics.comment": "label for area listing statistics about model prediction",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "helperText": "Evaluate the performance of your model by exploring the distribution of your prediction values and the values of your model performance metrics. You can further investigate your model by looking at a comparative analysis of its performance across different cohorts or subgroups of your dataset. Select filters along y-value and x-value to cut across different dimensions.",
      "missingParameters": "This tab requires the array of predicted values from the model be supplied.",
      "missingTrueY": "Model performance statistics require the true outcomes be provided in addition to the predicted outcomes",
      "modelStatistics": "Model statistics"
    },
    "PerturbationExploration": {
      "_perturbationLabel.comment": "Perturbation (ie. the user has made a set of small changes to the original data -- a perturbation. This is the label for the resulting prediction)",
      "loadingMessage": "Loading...",
      "perturbationLabel": "Perturbation:"
    },
    "PredictionLabel": {
      "_predictedClassLabel.comment": "label for prediction of class value",
      "_predictionValueLabel.comment": "label for prediction of numeric values",
      "predictedClassLabel": "Predicted class : {0}",
      "predictedValueLabel": "Predicted value : {0}"
    },
    "Statistics": {
      "_accuracy.comment": "computed accuracy of model on a subgroup, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "_fnr.comment": "False negative rate, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "_fpr.comment": "False positive rate, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "_meanPrediction.comment": "the average of all the predictions",
      "_mse.comment": "the mean squared error, see https://en.wikipedia.org/wiki/Mean_squared_error",
      "_precision.comment": "computed precision of model, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "_rSquared.comment": "the coefficient of determination, see https://en.wikipedia.org/wiki/Coefficient_of_determination",
      "_recall.comment": "computed recall of model, see https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers",
      "accuracy": "Accuracy: {0}",
      "bleuScore": "Bleu score: {0}",
      "bertScore": "Bert score: {0}",
      "exactMatchRatio": "Exact match ratio: {0}",
      "rougeScore": "Rouge Score: {0}",
      "fnr": "False negative rate: {0}",
      "fpr": "False positive rate: {0}",
      "hammingScore": "Hamming score: {0}",
      "meanPrediction": "Mean prediction {0}",
      "meteorScore": "Meteor Score: {0}",
      "mse": "Mean squared error: {0}",
      "precision": "Precision: {0}",
      "rSquared": "R²: {0}",
      "recall": "Recall: {0}",
      "selectionRate": "Selection rate: {0}",
      "mae": "Mean absolute error: {0}",
      "f1Score": "F1 score: {0}",
      "samples": "Sample size: {0}",
      "meanAveragePrecision": "Mean average precision: {0}",
      "averagePrecision": "Average precision: {0}",
      "averageRecall": "Average recall: {0}"
    },
    "ValidationErrors": {
      "_inconsistentDimensions.comment": "Raise warning if arguments passed in have different sizes, listing dimensions of both mismatching pieces.",
      "addFilters": "Add filters",
      "datasizeError": "The selected cohort is too large, please add filters to decrease the size of the cohort.",
      "datasizeWarning": "The evaluation dataset is too large to be effectively displayed in some charts, please add filters to decrease the size of the cohort. ",
      "errorHeader": "Some input parameters were inconsistent and will not be used: ",
      "evalData": "Evaluation dataset",
      "globalFeatureImportance": "Global feature importance",
      "inconsistentDimensions": "Inconsistent dimensions. {0} has dimensions {1}, expected {2}",
      "localFeatureImportance": "Local feature importance",
      "notArray": "{0} not an array. Expected array of dimension {1}",
      "notNonEmpty": "{0} input not a non-empty array",
      "predictedProbability": "Predicted probability",
      "predictedY": "Predicted Y",
      "varyingLength": "Inconsistent dimensions. {0} has elements of varying length"
    },
    "Violin": {
      "_groupBy.comment": "Group by prompt for dropdown to select how data should be grouped",
      "_groupName.comment": "Do not group data option",
      "_groupPredicted.comment": "option to group data by predicted class",
      "_groupTrue.comment": "option to group data by true class",
      "groupBy": "Group by",
      "groupNone": "No grouping",
      "groupPredicted": "Predicted Y",
      "groupTrue": "True Y"
    },
    "WhatIf": {
      "_defaultCustomRootName.comment": "default prefix for a hypothetical point made by copying another point",
      "_filterFeaturePlaceholder.comment": "placeholder in search box for searching for features by name",
      "closeAriaLabel": "Close",
      "defaultCustomRootName": "Copy of row {0}",
      "filterFeaturePlaceholder": "Search features"
    },
    "WhatIfTab": {
      "IceGetStartedText": "Select a point or create a What-If point to view ICE plots",
      "_IceGetStartedText.comment": "placeholder to prompt user to click point in neighboring chart to see this chart",
      "_classLabel.comment": "Prefix for class",
      "_cohortPickerLabel.comment": "label for single-select dropdown to pick a cohort to view",
      "_deltaLabel.comment": "represents the change in a value",
      "_disclaimer.comment": "the tool should not be liable for any bad predictions",
      "_featureImportanceGetStartedText.comment": "placeholder to prompt user to click point in neighboring chart to view this chart",
      "_featureImportanceLackingParameters.comment": "placeholder if no local feature importances passed in",
      "_featureImportancePlot.comment": "name of a plot type showing the importance of each feature in making a decision",
      "_featureValues.comment": "header above the list of feature names (column names)",
      "_helperText.comment": "explains what is shown in this tab and what actions are available",
      "_iceLackingParameters.comment": "placeholder text if a required parameter is not passed in, cannot show ICE plot if no model passed in",
      "_icePlot.comment": "name of a plot type named Individual Conditional Expectation (ICE)",
      "_indexLabel.comment": "label for dropdown for selecting a row by index value",
      "_loading.comment": "loading message while prediction is made",
      "_maxLabel.comment": "maximum (small space available)",
      "_minLabel.comment": "minimum (small space available)",
      "_missingParameters.comment": "Show a message if the required dataset parameter is not provided",
      "_newPredictedClass.comment": "the prediction after the user changed features",
      "_newPredictedValue.comment": "the prediction after the user changed features",
      "_newProbability.comment": "the probability for the new prediction",
      "_noneCreatedYet.comment": "placeholder if no hypothetical points have been created, goes where list of points would be",
      "_noneSelectedYet.comment": "placeholder if no points are selected, where list of points would go",
      "_panelPlaceholder.comment": "message shown to user when they did not give a model as inputs",
      "_predictedClass.comment": "the predicted class for a row",
      "_predictedValue.comment": "the predicted value for a row",
      "_probability.comment": "the probability that the predicted class is correct",
      "_realPoint.comment": "label above the list of points from the dataset",
      "_rowLabel.comment": "the label for a selected row, with its index number appended",
      "_saveAsNewPoint.comment": "button to save hypothetical point",
      "_saveChanges.comment": "button text to save changes made to a row",
      "_scatterLegendText.comment": "describes actions possible on the legend items in the chart",
      "_selectionLimit.comment": "A user can only select 3 points from a chart at a time, this message is displayed if they click a 4th",
      "_showLabel.comment": "label on button to pick what additional chart to show",
      "_stepsLabel.comment": "number of increments to use between minimum and maximum",
      "_tooltipTitleMany.comment": "placeholder is the number of classes shown",
      "_trueClass.comment": "prefix to actual label",
      "_whatIfDatapoint.comment": "header text for area where user writes a hypothetical point's values",
      "_whatIfDatapoints.comment": "label above the list of what if (hypothetical) points",
      "_whatIfHelpText.comment": "describe how to create a what-if hypothetical row",
      "_whatIfNameLabel.comment": "label above text field where user can name their what-if hypothetical point",
      "classLabel": "Class: {0}",
      "classPickerLabel": "Class",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "dataPointInfo": "Data point info",
      "deltaLabel": "Delta",
      "disclaimer": "Disclaimer: These are explanations based on many approximations and are not the \"cause\" of predictions. Without strict mathematical robustness of causal inference, we do not advise users to make real-life decisions based on this tool.",
      "featureImportanceGetStartedText": "Select a datapoint in the table above to view its local feature importances",
      "featureImportanceLackingParameters": "Provide local feature importances to see how each feature impacts individual predictions.",
      "featureImportancePlot": "Feature importance plot",
      "featureValues": "Feature values",
      "helperText": "Select an individual datapoint by clicking on a datapoint in the scatterplot to view its local feature importance values below and feature values in the panel on the right.",
      "iceLackingParameters": "ICE plots require an operationalized model to make predictions for hypothetical datapoints.",
      "icePlot": "Individual conditional expectation (ICE) plot",
      "icePlotHelperText": "ICE plots demonstrate how the selected datapoint's prediction values change along a range of feature values between a minimum and maximum value.",
      "indexLabel": "Data index",
      "loading": "Loading...",
      "localFeatureImportanceForPoint": "Local feature importance of selected data points",
      "maxLabel": "Max",
      "minLabel": "Min",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "newPredictedClass": "New predicted class: ",
      "newPredictedValue": "New predicted value: ",
      "newProbability": "New probability: ",
      "nonNumericValue": "Value should be numeric",
      "noneCreatedYet": "None created yet",
      "noneSelectedYet": "None selected yet",
      "notAvailable": "What-If is currently not supported in studio. Run this widget in a jupyter notebook to enable What-If.",
      "panelPlaceholder": "A model is required to make predictions for new data points.",
      "predictedClass": "Predicted class: ",
      "predictedValue": "Predicted value: ",
      "probability": "Probability: ",
      "probabilityLabel": "Probability",
      "realPoint": "Real datapoints",
      "rowLabel": "Row {0}",
      "saveAsNewPoint": "Save as new point",
      "saveChanges": "Save changes",
      "scatterLegendText": "Toggle datapoints on and off in the plot by clicking on the legend items.",
      "selectionLimit": "Maximum of 3 selected points",
      "showLabel": "Show:",
      "stepsLabel": "Steps",
      "tooltipTitleFew": "Predicted classes",
      "tooltipTitleMany": "Top {0} predicted classes",
      "trueClass": "True class: ",
      "trueValue": "True value: ",
      "trueValue.comment": "prefix to actual label for regression",
      "whatIfDatapoint": "What-If datapoint",
      "whatIfDatapoints": "What-If datapoints",
      "whatIfHelpText": "Select a point on the plot or manually enter a known datapoint index to perturb and save as a new What-If point.",
      "whatIfNameLabel": "What-If datapoint name",
      "whatIfTooltipTitle": "What-If predicted classes"
    },
    "_absoluteAverage.comment": "https://en.wikipedia.org/wiki/Norm_(mathematics) Absolute value norm",
    "_aggregateFeatureImportance.comment": "tab label for view of aggregated (summed and averaged) feature importances",
    "_dataExploration.comment": "Label for tab showing scatter plot of dataset and predictions",
    "_defaultClassNames.comment": " models that output classes have this as the default class names",
    "_defaultFeatureNames.comment": "the default column names",
    "_explanationExploration.comment": "Label for tab showing scatter plot of dataset and importance of features data",
    "_featureImportance.comment": "Label for feature importance ",
    "_globalImportance.comment": "Label for tab showing bar chart of importance of features at a global level",
    "_ice.comment": "Label for tab showing https://christophm.github.io/interpretable-ml-book/ice.html",
    "_individualAndWhatIf.comment": "tab label for feature importances of single rows, and 'what if?' which allows seeing hypothetical results",
    "_intercept.comment": "The label for the linear intercept, the bias value",
    "_localFeatureImportance.comment": "Label for local tab showing feature importance of selected point",
    "_modelPerformance.comment": "tab label to see how well a model predicted values when compared to true",
    "_perturbationExploration.comment": "Label for local tab allowing the user to change parameters on a selected point",
    "_predictedClass.comment": "Norm based on the predicted class rather than absolute value as above",
    "_selectPoint.comment": "Prompts the user to select a point",
    "_summaryImportance.comment": "Label showing all importance of feature data points in scatter plot",
    "absoluteAverage": "Average of absolute value",
    "aggregateFeatureImportance": "Aggregate feature importance",
    "calloutTitle": "Click for info",
    "clearSelection": "Clear selection",
    "dataExploration": "Dataset exploration",
    "datasetExplorer": "Dataset explorer",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Feature {0}",
    "explanationExploration": "Explanation exploration",
    "feature": "Feature:",
    "featureImportance": "Feature importance",
    "featureImportanceOf": "Feature importance of {0}",
    "globalImportance": "Global importance",
    "ice": "ICE",
    "individualAndWhatIf": "Individual feature importance & what-if",
    "individualImportance": "Individual feature importance",
    "intercept": "Intercept",
    "localFeatureImportance": "Local feature importance",
    "modelPerformance": "Model performance",
    "perturbationExploration": "Perturbation exploration",
    "predictedClass": "Predicted class",
    "selectPoint": "Select a point to see its local explanation",
    "summaryImportance": "Summary importance"
  },
  "InterpretText": {
    "View": {
      "interpretibilityDashboard": "Interpretability Dashboard",
      "importantWords": "Show most Important Words",
      "topFeatureList": "Top feature list analysis",
      "allButton": "ALL FEATURES",
      "negButton": "NEGATIVE FEATURES",
      "posButton": "POSITIVE FEATURES",
      "legendText": "Positive scalar feature importances represent the extent that the words were important towards the classification of your selected label, and negative scalar feature importances represent words that encouraged your model away from your selected label.",
      "label": "Label",
      "colon": ": "
    },
    "Legend": {
      "featureLegend": "TEXT FEATURE LEGEND",
      "posFeatureImportance": "POSITIVE FEATURE IMPORTANCE",
      "negFeatureImportance": "NEGATIVE FEATURE IMPORTANCE"
    },
    "BarChart": {
      "featureImportance": "FEATURE IMPORTANCE"
    }
  },
  "InterpretVision": {
    "Cohort": {
      "close": "Close",
      "errorCohortName": "Please choose a unique cohort name.",
      "errorNumSelected": "Please select at least one (1) item.",
      "itemsSelectedSingular": "item selected",
      "itemsSelectedPlural": "items selected",
      "save": "Save cohort",
      "saveAndClose": "Save and close",
      "saveAndSwitch": "Save and switch",
      "textField": "New cohort name",
      "title": "Save new cohort"
    },
    "Dashboard": {
      "allData": "All Data",
      "columnOne": "Image",
      "columnTwo": "Index",
      "columnThree": "True Y",
      "columnFour": "Predicted Y",
      "columnFive": "Other metadata",
      "chooseObject": "Choose a detected object",
      "examples": "examples",
      "filter": "Filter",
      "indexLabel": "Image ",
      "labelTypeDropdown": "Select label type",
      "labelVisibilityDropdown": "Select labels to display",
      "legendFailure": "failure",
      "legendSuccess": "success",
      "loading": "Computing explanation for index",
      "multiselect": "Multiselect",
      "notdefined": "object scenario not defined",
      "objectSelect": "Object Selection",
      "pageSize": "Page size: ",
      "panelTitle": "Selected instance",
      "panelExplanation": "Explanation",
      "panelInformation": "Information",
      "predictedLabel": "Predicted label",
      "predictedY": "Predicted: ",
      "prefix": "Object: ",
      "rows": "Rows: ",
      "search": "Search",
      "selectAll": "Select all",
      "settings": "Settings",
      "showAll": "Show all",
      "tabOptionFirst": "Image explorer view",
      "tabOptionSecond": "Table view",
      "tabOptionThird": "Class view",
      "thumbnailSize": "Thumbnail size",
      "titleBarError": "Error instances",
      "titleBarSuccess": "Success instances",
      "trueY": "Ground truth: "
    }
  },
  "ModelAssessment": {
    "AddingTab": {
      "AddButtonText": "Add",
      "CalloutContent": "Adding some components (error tree view, error heat map) will allow you to filter down the data from the global cohort that you see in the components below.",
      "CalloutTitle": "Add component",
      "TabAddedMessage": {
        "DataAnalysis": "Data analysis component added",
        "FeatureImportances": "Feature importances component added",
        "ErrorAnalysis": "Error analysis component added",
        "Fairness": "Fairness component added",
        "ModelOverview": "Model overview component added",
        "CausalAnalysis": "Causal analysis component added",
        "Counterfactuals": "Counterfactuals component added",
        "Vision": "Vision data explorer component added",
        "Forecasting": "Forecasting what-if component added"
      }
    },
    "CausalAnalysis": {
      "Table": {
        "ciLower": "Confidence interval (lower)",
        "ciUpper": "Confidence interval (upper)",
        "name": "Feature",
        "pValue": "P-value",
        "point": "Effect estimate",
        "stderr": "Standard error",
        "zstat": "Z-score"
      }
    },
    "Cohort": {
      "apply": "Apply",
      "delete": "Delete",
      "cancel": "Cancel",
      "cohortInfo": "Cohort info",
      "cohortList": "Cohort list",
      "deleteCohort": "Deleting {0}?",
      "deleteConfirm": "Are you sure you want to delete this cohort?",
      "selectCohort": "Select a cohort",
      "shiftCohort": "Switch cohort",
      "shiftCohortDescription": "Select a cohort from the cohort list. Apply the cohort to the dashboard."
    },
    "CohortInformation": {
      "ShiftCohort": "Switch cohort",
      "SwitchTimeSeries": "Switch time series",
      "NewCohort": "New cohort",
      "DataPoints": "Number of datapoints",
      "DefaultCohort": " (default)",
      "Filters": "Number of filters",
      "GlobalCohort": "Global cohort: "
    },
    "CohortSettings": {
      "CohortSettingsDescription": "Global cohorts are subsets of data created by manually adding filters or saving a selected group in the error analysis component. View, create, edit, duplicate, and delete cohorts.",
      "CohortSettingsTitle": "Cohort settings"
    },
    "ComponentNames": {
      "ChartView": "Chart view",
      "CausalAnalysis": "Causal analysis",
      "Counterfactuals": "Counterfactuals",
      "DataAnalysis": "Data analysis",
      "DataBalance": "Data balance",
      "DataExplorer": "Data explorer",
      "ErrorAnalysis": "Error analysis",
      "Fairness": "Fairness",
      "FeatureImportances": "Feature importances",
      "Forecasting": "Forecasting",
      "ModelOverview": "Model overview",
      "TableView": "Table view",
      "VisionTab": "Vision data explorer"
    },
    "DashboardSettings": {
      "Content": "This list shows the layout of the dashboard. You can filter down data using the error analysis component, to be viewed in the components below.",
      "DashboardComponents": "Dashboard components",
      "DataPoints": "Number of datapoints",
      "DeleteDialog": {
        "Cancel": "Cancel",
        "Content": "Are you sure you want to delete {0}?",
        "Delete": "Delete",
        "Title": "Delete {0}?"
      },
      "Title": "Dashboard configuration"
    },
    "DataBalance": {
      "AggregateBalanceMeasures": {
        "Callout": {
          "Description": "Aggregate balance measures allow us to obtain a higher notion of inequality about our dataset. They are calculated on the set of our columns of interest and do not depend on the label column. These measures look at the distribution of records across all feature value combinations of our columns of interest.",
          "Title": "What are aggregate balance measures?"
        },
        "Measures": {
          "AtkinsonIndex": {
            "Description": "The Atkinson Index is a measure of income inequality that presents the percentage of total income that a given society would have to forego in order to have more equal shares of income between its citizens. For the dataset, this represents the percentage of records that need to be dropped to have a perfectly balanced dataset with respect to the columns of interest. The index lies between 0 and 1, where smaller values of the index indicate lower levels of inequality.",
            "Name": "Atkinson Index"
          },
          "TheilLIndex": {
            "Description": "Theil's L Index is a measure of income inequality that is calculated using the Generalized Entropy Index with α = 0, and therefore it is more sensitive to differences at the lower end of the distribution. This index better captures record combinations that don't occur too often in the dataset. It is also referred to as the mean log deviation measure.",
            "Name": "Theil Index (L)"
          },
          "TheilTIndex": {
            "Description": "Theil's T Index is a measure of income inequality that is calculated using the Generalized Entropy Index with α = 1, and therefore it is more sensitive to differences at the higher end of the distribution. This index better captures record combinations that occur very often in the dataset.",
            "Name": "Theil Index (T)"
          }
        },
        "MeasuresNotComputed": "Aggregate balance measures were not computed. To compute them, please ensure that: (1) the task type is 'classification', (2) the list of categorical features is valid and non-empty, and (3) no errors are thrown when computing insights.",
        "Name": "Aggregate balance measures",
        "Table": {
          "Description": "Description",
          "FeatureName": "Feature name",
          "FeatureValue": "Feature value"
        }
      },
      "DistributionBalanceMeasures": {
        "Callout": {
          "Description": "Distribution balance measures allow us to compare our data with a reference uniform distribution. These measures are calculated per column of interest and do not depend on the label column.",
          "Title": "What are distribution balance measures?"
        },
        "Chart": {
          "Axes": {
            "DistributionMeasure": "Distribution measure",
            "MeasureValue": "Measure value"
          },
          "Title1": "Comparing",
          "Title2": "with the Uniform Distribution"
        },
        "Measures": {
          "ChiSquarePValue": {
            "Description": "The p-value of the chi-square test gives evidence against the null hypothesis that the difference in observed and expected frequencies is by random chance.",
            "Name": "Chi-Square P-Value"
          },
          "ChiSquareStatistic": {
            "Description": "The chi-square test validates the null hypothesis that the categorical data has the given frequencies when compared with the expected frequencies in each category.",
            "Name": "Chi-Square (χ2) Statistic"
          },
          "CrossEntropy": {
            "Description": "The cross-entropy, also known as log loss, between two probability distributions is a measure from information theory that computes the negative summation of our dataset's probability distribution multiplied by the log of the reference probability distribution.",
            "Name": "Cross Entropy"
          },
          "InfiniteNormDistance": {
            "Description": "The infinite norm distance, also called the Chebyshev distance or chessboard distance, is a measure which states that the distance between two vectors is the greatest of their difference along any coordinate dimension. The measure is non-negative, and a value of 0 means that the two distributions are equal.",
            "Name": "Infinite Norm Distance"
          },
          "JSDistance": {
            "Description": "The Jensen-Shannon (JS) distance measures the similarity between two probability distributions. It is the symmetrized and smoothed version of the Kullback-Leibler (KL) divergence and the square root of the JS divergence. Its range is [0, 1], in which a measure value of 0 means the distribution is perfectly balanced.",
            "Name": "Jensen-Shannon Distance"
          },
          "KLDivergence": {
            "Description": "The Kullback-Leibler (KL) divergence is a measure of how one probability distribution is different from a second probability distribution. It represents the amount of information lost when one distribution is used to approximate the other. The measure is non-negative, and a value of 0 means that the two distributions are equal.",
            "Name": "Kullback-Leibler Divergence"
          },
          "TotalVariationDistance": {
            "Description": "The total variation distance is equal to half the Manhattan (L1) distance between two distributions. It is computed using the summation of absolute value differences between two distributions. The measure is non-negative, and a value of 0 means that the two distributions are equal.",
            "Name": "Total Variation Distance"
          },
          "WassersteinDistance": {
            "Description": "The Wasserstein distance is also known as the earth mover's distance, since it can be seen as the minimum amount of 'work' required to transform one distribution into another. Work is measured as the amount of distribution weight that must be moved, multiplied by the distance it has to be moved. The measure is non-negative, and a value of 0 means that the two distributions are equal.",
            "Name": "Wasserstein Distance"
          }
        },
        "MeasuresNotComputed": "Distribution balance measures were not computed. To compute them, please ensure that: (1) the task type is 'classification', (2) the list of categorical features is valid and non-empty, and (3) no errors are thrown when computing insights.",
        "Name": "Distribution balance measures"
      },
      "FeatureBalanceMeasures": {
        "Callout": {
          "Description": "Feature balance measures allow us to see whether each combination of feature values is receiving the positive outcome (true prediction) at balanced probability.",
          "Title": "What are feature balance measures?"
        },
        "Chart": {
          "Axes": {
            "ClassA": "Class A",
            "ClassB": "Class B"
          },
          "Tooltip": {
            "And": "and",
            "ValueDescription1": "have a",
            "ValueDescription2": "of"
          }
        },
        "Description1": "Showing",
        "Description2": "gaps on all classes of",
        "Description3": "for label",
        "FeaturePicker": "Feature",
        "LabelPicker": "Positive label",
        "MeasurePicker": "Measure",
        "Measures": {
          "JaccardIndex": {
            "Description": "gap represents the difference between the Jaccard Index of Class A and Jaccard Index of Class B. Jaccard Index itself is an intersection-over-union measure and therefore has a range of [0, 1], where 0 means that the distributions are completely disjoint and 1 means they are completely identical.",
            "Name": "Jaccard Index"
          },
          "KendallRankCorrelation": {
            "Description": "(KRC) gap represents the difference between the KRC of Class A and KRC of Class B. Kendall Rank Correlation itself is a correlation statistic used to measure the ordinal association between two measured quantities, and the measure is high when observations have a similar rank and low when observations have a dissimilar rank between the two variables.",
            "Name": "Kendall Rank Correlation"
          },
          "LogLikelihoodRatio": {
            "Description": "(LLR) gap represents the difference between the LLR of Class A and LLR of Class B. Log Likelihood Ratio itself is a correlation statistic used to measure the ratio between the probability of a feature with the positive label and the probability of the positive label.",
            "Name": "Log-Likelihood Ratio"
          },
          "PointwiseMutualInformation": {
            "Description": "(PMI) gap represents the difference between the PMI of Class A and PMI of Class B. Pointwise Mutual Information itself is a entropy measure that, given Class A and Class B, quantifies the discrepancy between the probability of their coincidence given their joint distribution and their individual distributions (assuming independence).",
            "Name": "Pointwise Mutual Information"
          },
          "SorensenDiceCoefficient": {
            "Description": "(SDC) gap represents the difference between the SDC of Class A and SDC of Class B. Sørensen-Dice Coefficient itself is an intserction-over-union measure that is used to guage the similarity of two samples. It is related to the F1 score, and it equals twice the number of elements common to both sets divided by the sum of the number of elements in each set.",
            "Name": "Sørensen-Dice Coefficient"
          },
          "StatisticalParity": {
            "Description": "(SP) gap represents the difference between the SP of Class A and SP of Class B. Statistical Parity itself measures the positive rate, which is how much one class receives the positive outcome. The gap measures the difference between the two classes' positive rates. As close to 0 means both classes receive the positive outcome equally. A positive value means the first class sees the positive label more than the second class. A negative value means the opposite.",
            "Name": "Statistical Parity"
          },
          "TTest": {
            "Description": "gap represents the difference between the t-test of Class A and t-test of Class B. t-test itself is a correlation statistic used to compare the means of two groups (pairwise). The gap value is looked up on the t-distribution table to tell if the two groups are significantly different.",
            "Name": "t-test"
          },
          "TTestPValue": {
            "Description": "gap represents the statistical signifiance of the t-test gap between Class A and Class B. The t-test gap value and degrees of freedom are used to calculate the p-value.",
            "Name": "t-test p-value"
          }
        },
        "MeasuresNotComputed": "Feature balance measures were not computed. To compute them, please ensure that: (1) the task type is 'classification', (2) the list of categorical features is valid and non-empty, and (3) no errors are thrown when computing insights.",
        "Name": "Feature balance measures"
      },
      "LearnMore": "Learn more",
      "MeasuresNotComputed": "This tab requires data balance measures to be computed for this dataset. Measures are computed when a 'classification' task type and a list of non-empty categorical features are specified."
    },
    "FeatureImportances": {
      "CorrectPredictions": "Correct predictions",
      "GlobalExplanation": "Aggregate feature importance",
      "IncorrectPredictions": "Incorrect predictions",
      "InfoTitle": "Additional information on data analysis table view",
      "IndividualFeatureTabular": "Select a datapoint by clicking on a datapoint (up to 5 datapoints) in the table to view their local feature importance values (local explanation) and individual conditional expectation (ICE) plots.",
      "IndividualFeatureText": "Select a datapoint by clicking on a datapoint in the table to view the local feature importance values (local explanation).",
      "LocalExplanation": "Individual feature importance",
      "SelectionCounter": "{0}/{1} datapoints selected",
      "SelectionLimit": "Up to 5 datapoints can be selected at this time.",
      "RowCheckboxAriaLabel": "Row checkbox",
      "SelectionColumnAriaLabel": "Toggle selection"
    },
    "IndividualFeatureImportanceView": {
      "SmallInstanceSelection": "Instance selection"
    },
    "MainMenu": {
      "DashboardSettings": "Dashboard configuration",
      "cohortInfo": "Cohort info",
      "cohortList": "Cohort list",
      "cohortSettings": "Cohort settings",
      "createCohort": "Create cohort",
      "shiftCohort": "Switch cohort"
    },
    "Navigation": {
      "modelStatistics": "Model statistics"
    },
    "ModelOverview": {
      "metrics": {
        "accuracy": {
          "name": "Accuracy score",
          "description": "The fraction of data points classified correctly."
        },
        "exactMatchRatio": {
          "name": "Exact match ratio",
          "description": "The ratio of instances classified correctly for every label."
        },
        "meteorScore": {
          "name": "Meteor Score",
          "description": "METEOR Score is calculated based on the harmonic mean of precision and recall, with recall weighted more than precision in question answering task."
        },
        "bleuScore": {
          "name": "Bleu Score",
          "description": "Bleu Score measures the ratio of words (and/or n-grams) in the machine generated text that appeared in the reference text in question answering task."
        },
        "bertScore": {
          "name": "Bert Score",
          "description": "BERTScore focuses on computing semantic similarity between tokens of reference and machine generated text in question answering task."
        },
        "rougeScore": {
          "name": "Rouge Score",
          "description": "Rouge Score measures the ratio of words (and/or n-grams) in the reference text that appeared in the machine generated text in question answering task."
        },
        "hammingScore": {
          "name": "Hamming score",
          "description": "The average ratio of labels classified correctly among those classified as 1 in multilabel task."
        },
        "f1Score": {
          "name": "F1 score",
          "description": "F1-score is the harmonic mean of precision and recall."
        },
        "f1ScoreMacro": {
          "name": "Macro F1 score",
          "description": "Macro F1 score is the harmonic mean of precision and recall for each class, with each class weighted equally."
        },
        "f1ScoreMicro": {
          "name": "Micro F1 score",
          "description": "Micro F1 score is the harmonic mean of precision and recall for each class, with each class weighted according to how many instances it contains."
        },
        "meanAbsoluteError": {
          "name": "Mean absolute error",
          "description": "The average of absolute values of errors. More robust to outliers than MSE."
        },
        "meanSquaredError": {
          "name": "Mean squared error",
          "description": "The average of squared errors."
        },
        "precision": {
          "name": "Precision score",
          "description": "The fraction of data points classified correctly among those classified as 1."
        },
        "precisionMacro": {
          "name": "Macro Precision score",
          "description": "The fraction of data points classified correctly among those classified as 1 for each class with each class weighted equally."
        },
        "precisionMicro": {
          "name": "Micro Precision score",
          "description": "The fraction of data points classified correctly among those classified as 1 for each class with each class weighted according to how many instances it contains."
        },
        "recall": {
          "name": "Recall score",
          "description": "The fraction of data points classified correctly among those whose true label is 1. Alternative names: true positive rate, sensitivity."
        },
        "recallMacro": {
          "name": "Macro Recall score",
          "description": "The fraction of data points classified correctly among those whose true label is 1 for each class with each class weighted equally."
        },
        "recallMicro": {
          "name": "Micro Recall score",
          "description": "The fraction of data points classified correctly among those whose true label is 1 for each class with each class weighted according to how many instances it contains."
        },
        "falsePositiveRate": {
          "name": "False positive rate",
          "description": "The fraction of data points classified incorrectly among those whose true label is 0."
        },
        "falseNegativeRate": {
          "name": "False negative rate",
          "description": "The fraction of data points classified incorrectly among those whose true label is 1."
        },
        "selectionRate": {
          "name": "Selection rate",
          "description": "The percentage of predictions with label 1."
        },
        "meanPrediction": {
          "name": "Mean prediction",
          "description": "The average of all predictions."
        },
        "meanAveragePrecision": {
          "name": "Mean Average Precision score",
          "description": "Mean average precision for object detection models is the average of AP (average precision) across all classes. This evaluates the robustness of your object detection model and encapsulates the tradeoff between precision and recall."
        },
        "averagePrecision": {
          "name": "Average Precision score",
          "description": "Average precision for object detection models is calculated for a selected class."
        },
        "averageRecall": {
          "name": "Average Recall score",
          "description": "Average recall for object detection models is calculated for a selected class."
        },
        "fairnessMetricDifference": "Difference",
        "fairnessMetricRatio": "Ratio"
      },
      "metricsDropdown": "Metric(s)",
      "metricsTypeDropdown": "Aggregate method",
      "metricTypes": {
        "macro": "Macro",
        "micro": "Micro"
      },
      "classSelectionDropdown": "Select class(es)",
      "iouThresholdDropdown": {
        "name": "IoU Threshold",
        "description": "Intersection over Union quantifies the degree of overlap between the prediction and ground truth bounding box of a detected object in an image. For example, setting an IoU threshold of 70% means that a prediction with greater than 70% overlap with ground truth is True, thus influencing the definition of prediction correctness and calculation of other performance metrics.",
        "iconId": "iouThresholdIconId",
        "title": "Learn about the IoU threshold"
      },
      "notAvailable": "N/A",
      "countColumnHeader": "Sample size",
      "dataCohortsHeatmapHeader": "Cohorts",
      "dataCohortsChartSelectionHeader": "Dataset cohorts",
      "featureBasedCohortsChartSelectionHeader": "Feature cohorts",
      "disaggregatedAnalysisHeatmapHeader": "Feature cohorts",
      "fairnessMetricsHeatmapHeader": "Fairness metrics",
      "featuresDropdown": "Feature(s)",
      "metricChartDropdownSelectionHeader": "Metric",
      "probabilityForClassSelectionHeader": "Probability for class",
      "targetSelectionHeader": "Target",
      "metricSelectionDropdownPlaceholder": "Select metrics to compare your cohorts.",
      "classSelectionDropdownPlaceholder": "Select class name for class-based analysis.",
      "featureSelectionDropdownPlaceholder": "Select features to use for a feature-based analysis.",
      "probabilityDistributionPivotItem": "Probability distribution",
      "regressionDistributionPivotItem": "Target distribution",
      "metricsVisualizationsPivotItem": "Metrics visualizations",
      "confusionMatrixPivotItem": "Confusion matrix",
      "disaggregatedAnalysisFeatureSelectionPlaceholder": "Select features to generate the feature-based analysis.",
      "tableCountTooltip": "Cohort {0} contains {1} instances.",
      "tableMetricTooltip": "The model's {0} on cohort {1} is {2}",
      "tableDifferenceTooltip": "The {0} difference is {1}. It is calculated by subtracting the minimum value {2} from cohort {3} from the maximum value {4} from cohort {5}.",
      "tableRatioTooltip": "The {0} ratio is {1}. It is calculated by dividing the minimum value {2} from cohort {3} by the maximum value {4} from cohort {5}.",
      "boxPlotPlaceholder": "Select at least one cohort to view the box plot.",
      "chartCohortSelectionPlaceholder": "Select at least one cohort overall to view the chart.",
      "metricSelectionButton": "Choose metric",
      "cohortSelectionButton": "Choose cohorts",
      "probabilityLabelSelectionButton": "Choose label",
      "regressionTargetSelectionButton": "Choose target",
      "selectAllCohortsOption": "Select all",
      "other": "Other",
      "BoxPlot": {
        "outlierProbability": "probability",
        "outlierLabel": "Outliers",
        "boxPlotSeriesLabel": "Box Plot",
        "lowerWhisker": "Lower whisker",
        "upperWhisker": "Upper whisker",
        "median": "Median",
        "lowerQuartile": "Lower quartile",
        "upperQuartile": "Upper quartile"
      },
      "chartConfigApply": "Apply",
      "chartConfigCancel": "Cancel",
      "chartConfigDatasetCohortSelectionPlaceholder": "Select dataset cohorts",
      "chartConfigFeatureBasedCohortSelectionPlaceholder": "Select feature-based cohorts",
      "confusionMatrix": {
        "confusionMatrixCohortSelectionLabel": "Select dataset cohort",
        "confusionMatrixClassSelectionLabel": "Select classes",
        "confusionMatrixClassMinSelectionError": "Select at least {0} classes to visualize the confusion matrix.",
        "confusionMatrixClassMaxSelectionError": "Select at most {0} classes to visualize the confusion matrix.",
        "confusionMatrixClassSelectionDefaultPlaceholder": "Choose classes",
        "confusionMatrixHeatmapTooltip": "{0} datapoints should be {1}, predicted to be {2}",
        "confusionMatrixYAxisLabel": "True Class",
        "confusionMatrixXAxisLabel": "Predicted Class",
        "class": "Class"
      },
      "nA": "N/A",
      "disaggregatedAnalysisBaseCohortDisclaimer": "The cohorts in the following feature-based analysis are based on the global cohort, {0}.",
      "disaggregatedAnalysisBaseCohortWarning": "Unlike the {0} cohort, {1} includes filters. As a consequence, it only captures a subset of the whole dataset and insights may not generalize to the full dataset.",
      "probabilitySplineChartToggleLabel": "Use spline chart",
      "countAxisLabel": "Count",
      "helpMeChooseFeaturesButton": "Help me choose features",
      "helpMeChooseMetricsButton": "Help me choose metrics",
      "featureConfiguration": {
        "flyoutHeader": "Choose your features",
        "flyoutSubHeader": "Along which features would you like to evaluate your model's fairness?",
        "flyoutDescription": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's performance metrics differ across these splits.",
        "featureColumnHeader": "Features",
        "groupsColumnHeader": "Groups",
        "categoricalGroupsCountRemark": "This feature has {0} unique values.",
        "continuousGroupsCountRemark": "This feature is continuous and split into {0} bins.",
        "continuousFeatureBinningLabel": "Change the number of bins",
        "tooManyFeaturesSelectedWarning": "Please select only up to two features."
      },
      "metricConfiguration": {
        "flyoutHeader": "Choose your metrics",
        "flyoutSubHeader": "How would you like to measure your model's performance?",
        "flyoutDescription": "Based on your model type, these are the recommended performance metrics. Fairness analysis or disaggregated analysis is performed by evaluating how your model's performance differs across feature subgroups.",
        "noMetricsSelectedWarning": "Please select at least one metric.",
        "metricNameColumnHeader": "Metric name",
        "metricDescriptionColumnHeader": "Metric Description"
      },
      "cohortSelection": {
        "flyoutHeader": "Choose cohorts",
        "flyoutDescription": "You can choose to view dataset cohorts or feature cohorts. If feature cohorts are unavailable you need to first select one or more features in the feature cohorts view. Subsequently, feature cohorts are generated and you can select them here."
      },
      "regressionTargetOptions": {
        "predictedY": "Predicted Y",
        "trueY": "True Y",
        "error": "Error"
      },
      "topLevelDescription": "Evaluate the performance of your model by exploring the distribution of your prediction values and the values of your model performance metrics. Use the \"Dataset cohorts\" tab to investigate your model by looking at a comparative analysis of its performance across different pre-built or newly-created dataset cohorts. Use the \"Feature cohorts\" to investigate your model by looking at a comparative analysis of its performance across sensitive/non-sensitive feature subcohorts.(e.g., performance across different genders, income levels).",
      "infoTitle": "Additional information on model overview",
      "visualDisplayToggleLabel": "Show heatmap",
      "featureBasedViewDescription": "Select up to two features to see the model performance breakdown across feature-based cohorts (if one feature is selected) or intersectional cohorts (if two features are selected)."
    },
    "TableViewTab": {
      "Heading": "View the dataset in a table format for all features and rows."
    }
  },
  "Forecasting": {
    "target": "Target",
    "whatIfForecastingHeader": "What-if analysis",
    "forecastHeader": "Forecast analysis",
    "whatIfForecastingDescription": "What-if allows you to perturb features for your entire time series and observe how the model's forecast changes.",
    "whatIfForecastingChooseTimeSeries": "To start, choose a time series from the options below.",
    "forecastDescription": "Forecast analysis compares your model's forecast to the actual values of your time series. To enable what-if analysis, provide a dataset with features.",
    "timeSeries": "Time series",
    "selectTimeSeries": "Select a time series.",
    "singleTimeSeries": "The dataset contains only a single time series '{0}' which has been selected by default.",
    "trueY": "True Y",
    "baselinePrediction": "Baseline prediction",
    "forecastComparisonHeader": "Compare What-if Forecasts",
    "forecastComparisonChartTitle": "Forecasts",
    "forecastComparisonChartTimeAxisLabel": "Time",
    "Transformations": {
      "multiply": "multiply",
      "divide": "divide",
      "add": "add",
      "subtract": "subtract",
      "change": "change to"
    },
    "TransformationCreation": {
      "title": "Create what-if scenario",
      "nameLabel": "What-if scenario name",
      "featureInstructions": "Choose a feature to perturb.",
      "operationInstructions": "Choose an operation to apply to the feature.",
      "operationDropdownHeader": "Operation",
      "featureDropdownHeader": "Feature",
      "valueSpinButtonHeader": "Value",
      "scenarioNamingInstructionsPlaceholder": "Enter a unique name",
      "scenarioNamingInstructions": "Enter a name for your what-if scenario.",
      "scenarioNamingCollisionMessage": "This name exists already. Please enter a unique name.",
      "scenarioNamingLengthMessage": "The name must be between 1 and 50 characters. The actual length is {0}.",
      "scenarioNamingInvalidCharactersMessage": "The name can only contain alphanumeric characters, whitespaces, dashes, or underscores, and needs to start with an alphanumeric character.",
      "valueErrorMessage": "For operation {0} please select a value other than {1}.",
      "invalidCombinationErrorMessage": "This is identical to an existing what-if scenario. Please change the feature, operation, or value.",
      "addTransformationButton": "Add Transformation",
      "divisionAndMultiplicationBy": "by"
    },
    "TransformationTable": {
      "nameColumnHeader": "Name",
      "methodColumnHeader": "Method",
      "divisionAndMultiplicationBy": "by ",
      "header": "What-if Forecasts ({0})"
    },
    "TimeSeries": {
      "apply": "Apply",
      "cancel": "Cancel",
      "cohortList": "Time series list",
      "selectCohort": "Select a time series",
      "shiftCohort": "Switch time series",
      "shiftCohortDescription": "Select a time series from the time series list. Apply the time series to the dashboard."
    },
    "TimeSeriesSettings": {
      "CohortSettingsDescription": "Time series are pre-defined based on time series identifying columns.",
      "CohortSettingsTitle": "Time series settings"
    }
  }
}
