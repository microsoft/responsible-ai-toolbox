{
  "Core": {
    "ExpandableText": {
      "SeeMore": "See more",
      "SeeLess": "See less"
    }
  },
  "Interpret": {
    "calloutTitle": "Click for info",
    "selectPoint": "Select a point to see its local explanation",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Feature {0}",
    "absoluteAverage": "Average of absolute value",
    "predictedClass": "Predicted class",
    "datasetExplorer": "Dataset explorer",
    "dataExploration": "Dataset exploration",
    "aggregateFeatureImportance": "Aggregate feature importance",
    "globalImportance": "Global importance",
    "explanationExploration": "Explanation exploration",
    "individualImportance": "Individual feature importance",
    "individualAndWhatIf": "Individual feature importance & what-if",
    "summaryImportance": "Summary importance",
    "featureImportance": "Feature importance",
    "featureImportanceOf": "Feature importance of {0}",
    "perturbationExploration": "Perturbation exploration",
    "localFeatureImportance": "Local feature importance",
    "ice": "ICE",
    "clearSelection": "Clear selection",
    "feature": "Feature:",
    "intercept": "Intercept",
    "modelPerformance": "Model performance",
    "ExplanationScatter": {
      "dataLabel": "Data : {0}",
      "importanceLabel": "Importance : {0}",
      "predictedY": "Predicted Y",
      "index": "Index",
      "dataGroupLabel": "Data",
      "output": "Output",
      "probabilityLabel": "Probability : {0}",
      "trueY": "True Y",
      "class": "class: ",
      "xValue": "X value:",
      "yValue": "Y value:",
      "colorValue": "Color:",
      "count": "Count"
    },
    "CrossClass": {
      "label": "Cross-class weighting:",
      "info": "Information on cross-class calculation",
      "overviewInfo": "Multiclass models generate an independent feature importance vector for each class. Each class's feature importance vector demonstrates which features made a class more likely or less likely. You can select how the weights of the per-class feature importance vectors are summarized into a single value:",
      "absoluteValInfo": "Average of absolute value: Shows the sum of the feature's importance across all possible classes, divided by number of classes",
      "predictedClassInfo": "Predicted class: Shows the feature importance value for a given point's predicted class",
      "enumeratedClassInfo": "Enumerated class names: Shows only the specified class's feature importance values across all data points.",
      "close": "Close",
      "crossClassWeights": "Cross class weights"
    },
    "AggregateImportance": {
      "scaledFeatureValue": "Scaled feature value",
      "low": "Low",
      "high": "High",
      "featureLabel": "Feature: {0}",
      "valueLabel": "Feature value: {0}",
      "importanceLabel": "Importance: {0}",
      "predictedClassTooltip": "Predicted class: {0}",
      "trueClassTooltip": "True class: {0}",
      "predictedOutputTooltip": "Predicted output: {0}",
      "trueOutputTooltip": "True output: {0}",
      "topKFeatures": "Top K Features:",
      "topKInfo": "How top k is calculated",
      "predictedValue": "Predicted value",
      "predictedClass": "Predicted class",
      "trueValue": "True value",
      "trueClass": "True class",
      "noColor": "None",
      "tooManyRows": "The provided dataset is larger than this chart can support"
    },
    "BarChart": {
      "classLabel": "Class: {0}",
      "sortBy": "Sort by",
      "noData": "No data",
      "absoluteGlobal": "Absolute global",
      "absoluteLocal": "Absolute local",
      "calculatingExplanation": "Calculating explanation"
    },
    "IcePlot": {
      "numericError": "Must be numeric",
      "integerError": "Must be an integer",
      "prediction": "Prediction",
      "predictedProbability": "Predicted probability",
      "predictionLabel": "Prediction: {0}",
      "probabilityLabel": "Probability: {0}",
      "noModelError": "Please provide an operationalized model to explore predictions in ICE plots.",
      "featurePickerLabel": "Feature:",
      "minimumInputLabel": "Minimum:",
      "maximumInputLabel": "Maximum:",
      "stepInputLabel": "Steps:",
      "loadingMessage": "Loading data...",
      "submitPrompt": "Submit a range to view an ICE plot",
      "topLevelErrorMessage": "Error in parameter",
      "errorPrefix": "Error encountered: {0}"
    },
    "PerturbationExploration": {
      "loadingMessage": "Loading...",
      "perturbationLabel": "Perturbation:"
    },
    "PredictionLabel": {
      "predictedValueLabel": "Predicted value : {0}",
      "predictedClassLabel": "Predicted class : {0}"
    },
    "Violin": {
      "groupNone": "No grouping",
      "groupPredicted": "Predicted Y",
      "groupTrue": "True Y",
      "groupBy": "Group by"
    },
    "FeatureImportanceWrapper": {
      "chartType": "Chart type:",
      "violinText": "Violin",
      "barText": "Bar",
      "boxText": "Box",
      "beehiveText": "Swarm",
      "globalImportanceExplanation": "Global feature importance is calculated by averaging the absolute value of the feature importance of all points (L1 normalization). ",
      "multiclassImportanceAddendum": "All points are included in calculating a feature's importance for all classes, no differential weighting is used. So a feature that has large negative importance for many points predicted to not be of 'Class A' will greatly increase that feature's 'Class A'  importance."
    },
    "Filters": {
      "equalComparison": "Equal to",
      "greaterThanComparison": "Greater than",
      "greaterThanEqualToComparison": "Greater than or equal to",
      "lessThanComparison": "Less than",
      "lessThanEqualToComparison": "Less than or equal to",
      "inTheRangeOf": "In the range of",
      "categoricalIncludeValues": "Included values:",
      "numericValue": "Value",
      "numericalComparison": "Operation",
      "minimum": "Minimum",
      "maximum": "Maximum",
      "min": "Min: {0}",
      "max": "Max: {0}",
      "uniqueValues": "# of unique values: {0}"
    },
    "Columns": {
      "regressionError": "Regression error",
      "error": "Error",
      "classificationOutcome": "Classification outcome",
      "truePositive": "True positive",
      "trueNegative": "True negative",
      "falsePositive": "False positive",
      "falseNegative": "False negative",
      "dataset": "Dataset",
      "predictedProbabilities": "Prediction probabilities",
      "none": "Count"
    },
    "WhatIf": {
      "closeAriaLabel": "Close",
      "defaultCustomRootName": "Copy of row {0}",
      "filterFeaturePlaceholder": "Search features"
    },
    "Cohort": {
      "cohort": "Cohort",
      "defaultLabel": "All data"
    },
    "GlobalTab": {
      "collapsedHelperText": "Explore the top k important features that impact your overall model predictions.",
      "helperText": "Use the slider to show descending feature importances . Select up to three cohorts to see their feature importances side by side. Click on any of the features in the graph to see a density plot below of how values of the selected feature affect prediction.",
      "topAtoB": "Top {0}-{1} features",
      "datasetCohorts": "Dataset cohorts",
      "legendHelpText": "Toggle cohorts on and off in the plot by clicking on the legend items.",
      "sortBy": "Sort by",
      "viewDependencePlotFor": "View dependence plot for:",
      "datasetCohortSelector": "Select a dataset cohort",
      "aggregateFeatureImportance": "Aggregate feature importance",
      "missingParameters": "This tab requires the local feature importance parameter be supplied.",
      "weightOptions": "Class importance weights",
      "dependencePlotTitle": "Dependence plots",
      "dependencePlotHelperText": "This dependence plot shows the relationship between the value of a feature to the corresponding importance of the feature across a cohort.",
      "dependencePlotFeatureSelectPlaceholder": "Select feature",
      "datasetRequired": "Dependence plots require the evaluation dataset and local feature importance array."
    },
    "CohortBanner": {
      "details": "Details",
      "name": "Name",
      "dataStatistics": "Data statistics",
      "datapoints": "{0} datapoints",
      "features": "{0} features",
      "filters": "{0} filters",
      "binaryClassifier": "Binary classifier",
      "regressor": "Regressor",
      "multiclassClassifier": "Multiclass classifier",
      "datasetCohorts": "Dataset cohorts",
      "editCohort": "Edit cohort",
      "duplicateCohort": "Duplicate",
      "addCohort": "New cohort",
      "copy": " copy"
    },
    "ModelPerformance": {
      "collapsedHelperText": "Evaluate the performance of your model by exploring the distribution of your prediction values and the values of your model performance metrics.",
      "helperText": "You can further investigate your model by looking at a comparative analysis of its performance across different cohorts or subgroups of your dataset. Select filters along y-value and x-value to cut across different dimensions. Select the gear in the graph to change graph type.",
      "modelStatistics": "Model statistics",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "missingParameters": "This tab requires the array of predicted values from the model be supplied.",
      "missingTrueY": "Model performance statistics require the true outcomes be provided in addition to the predicted outcomes"
    },
    "Charts": {
      "yValue": "Y-value",
      "numberOfDatapoints": "Number of datapoints",
      "xValue": "X-value",
      "rowIndex": "Row index",
      "featureImportance": "Feature importance",
      "countTooltipPrefix": "Count: {0}",
      "count": "Count",
      "featurePrefix": "Feature",
      "importancePrefix": "Importance",
      "cohort": "Cohort",
      "howToRead": "How to read this chart"
    },
    "DatasetExplorer": {
      "collapsedHelperText": "Learn about the over/underpresentation in your dataset.",
      "helperText": "Create dataset cohorts to the left to analyze statistics along filters such as predicted outcome, dataset features and error groups. Use the x-axis and color selectors to further slice your dataset into more dimensions.",
      "colorValue": "Color value",
      "individualDatapoints": "Individual datapoints",
      "aggregatePlots": "Aggregate plots",
      "chartType": "Chart type",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "noColor": "None"
    },
    "DependencePlot": {
      "featureImportanceOf": "Feature importance of",
      "placeholder": "Click on a feature on the bar chart above to show its dependence plot"
    },
    "WhatIfTab": {
      "helperText": "Select an individual datapoint by cllicking on a datapoint in the scatterplot to view its local feature importance values below and feature values in the panel on the right.",
      "panelPlaceholder": "A model is required to make predictions for new data points.",
      "cohortPickerLabel": "Select a dataset cohort to explore",
      "scatterLegendText": "Toggle datapoints on and off in the plot by clicking on the legend items.",
      "realPoint": "Real datapoints",
      "noneSelectedYet": "None selected yet",
      "dataPointInfo": "Data point info",
      "whatIfDatapoints": "What-If datapoints",
      "noneCreatedYet": "None created yet",
      "showLabel": "Show:",
      "localFeatureImportanceForPoint": "Local feature importance of selected data points",
      "featureImportancePlot": "Feature importance plot",
      "icePlot": "Individual conditional expectation (ICE) plot",
      "featureImportanceLackingParameters": "Provide local feature importances to see how each feature impacts individual predictions.",
      "featureImportanceGetStartedText": "Select a point to see its local importance",
      "iceLackingParameters": "ICE plots require an operationalized model to make predictions for hypothetical datapoints.",
      "IceGetStartedText": "Select a point or create a What-If point to view ICE plots",
      "whatIfDatapoint": "What-If datapoint",
      "whatIfHelpText": "Select a point on the plot or manually enter a known datapoint index to perturb and save as a new What-If point.",
      "notAvailable": "What-If is currently not supported in studio. Run this widget in a jupyter notebook to enable What-If.",
      "indexLabel": "Data index",
      "rowLabel": "Row {0}",
      "whatIfNameLabel": "What-If datapoint name",
      "featureValues": "Feature values",
      "predictedClass": "Predicted class: ",
      "predictedValue": "Predicted value: ",
      "probability": "Probability: ",
      "trueClass": "True class: ",
      "trueValue": "True value: ",
      "trueValue.comment": "prefix to actual label for regression",
      "newPredictedClass": "New predicted class: ",
      "newPredictedValue": "New predicted value: ",
      "newProbability": "New probability: ",
      "saveAsNewPoint": "Save as new point",
      "saveChanges": "Save changes",
      "loading": "Loading...",
      "classLabel": "Class: {0}",
      "minLabel": "Min",
      "maxLabel": "Max",
      "stepsLabel": "Steps",
      "disclaimer": "Disclaimer: These are explanations based on many approximations and are not the \"cause\" of predictions. Without strict mathematical robustness of causal inference, we do not advise users to make real-life decisions based on this tool.",
      "missingParameters": "This tab requires an evaluation dataset be supplied.",
      "selectionLimit": "Maximum of 3 selected points",
      "classPickerLabel": "Class",
      "tooltipTitleMany": "Top {0} predicted classes",
      "whatIfTooltipTitle": "What-If predicted classes",
      "tooltipTitleFew": "Predicted classes",
      "probabilityLabel": "Probability",
      "deltaLabel": "Delta",
      "nonNumericValue": "Value should be numeric",
      "icePlotHelperText": "ICE plots demonstrate how the selected datapoint's prediction values change along a range of feature values between a minimum and maximum value."
    },
    "CohortEditor": {
      "selectFilter": "Select filter",
      "TreatAsCategorical": "Treat as categorical",
      "addFilter": "Add filter",
      "addedFilters": "Added filters",
      "noAddedFilters": "No filters added yet",
      "defaultFilterState": "Select a filter to add parameters to your dataset cohort.",
      "cohortNameLabel": "Dataset cohort name",
      "cohortNamePlaceholder": "Name your cohort",
      "save": "Save",
      "delete": "Delete",
      "cancel": "Cancel",
      "cohortNameError": "Missing cohort name",
      "placeholderName": "Cohort {0}",
      "cancelTitle": "Cancel cohort",
      "cancelNewCohort": "Are you sure you want to cancel creating a new cohort and go back?",
      "cancelExistingCohort": "Are you sure you want to cancel editing cohort and go back?",
      "cancelYes": "Yes",
      "cancelNo": "No"
    },
    "AxisConfigDialog": {
      "select": "Select",
      "ditherLabel": "Should dither",
      "selectFilter": "Select your axis value",
      "selectFeature": "Select feature",
      "binLabel": "Apply binning to data",
      "TreatAsCategorical": "Treat as categorical",
      "numOfBins": "Number of bins",
      "groupByCohort": "Group by cohort",
      "selectClass": "Select class",
      "countHelperText": "A histogram of the number of points"
    },
    "ValidationErrors": {
      "predictedProbability": "Predicted probability",
      "predictedY": "Predicted Y",
      "evalData": "Evaluation dataset",
      "globalFeatureImportance": "Global feature importance",
      "localFeatureImportance": "Local feature importance",
      "inconsistentDimensions": "Inconsistent dimensions. {0} has dimensions {1}, expected {2}",
      "notNonEmpty": "{0} input not a non-empty array",
      "varyingLength": "Inconsistent dimensions. {0} has elements of varying length",
      "notArray": "{0} not an array. Expected array of dimension {1}",
      "errorHeader": "Some input parameters were inconsistent and will not be used: ",
      "datasizeWarning": "The evaluation dataset is too large to be effectively displayed in some charts, please add filters to decrease the size of the cohort. ",
      "datasizeError": "The selected cohort is too large, please add filters to decrease the size of the cohort.",
      "addFilters": "Add filters"
    },
    "FilterOperations": {
      "equals": " = {0}",
      "lessThan": " < {0}",
      "greaterThan": " > {0}",
      "lessThanEquals": " <= {0}",
      "greaterThanEquals": " >= {0}",
      "includes": " includes {0} ",
      "excludes": " excludes {0} ",
      "inTheRangeOf": "[ {0} ]",
      "overflowFilterArgs": "{0} and {1} others"
    },
    "Statistics": {
      "mse": "MSE: {0}",
      "rSquared": "R-squared: {0}",
      "meanPrediction": "Mean prediction {0}",
      "accuracy": "Accuracy: {0}",
      "precision": "Precision: {0}",
      "recall": "Recall: {0}",
      "fpr": "FPR: {0}",
      "fnr": "FNR: {0}"
    },
    "GlobalOnlyChart": {
      "helperText": "Explore the top k important features that impact your overall model predictions. Use the slider to show descending feature importances."
    },
    "ExplanationSummary": {
      "whatDoExplanationsMean": "What do these explanations mean?",
      "clickHere": "Learn more",
      "shapTitle": "Shapley values",
      "shapDescription": "This explainer uses SHAP, which is a game theoretic approach to explaining models, where the importance of features sets is measured by \"hiding\" those features from the model through marginalization. Click the link below to learn more.",
      "limeTitle": "LIME (Local Interpretable Model-Agnostic Explanations)",
      "limeDescription": "This explainer uses LIME, which provides a linear approximation of the model. To get an explanation, we do the following: perturb the instance, get model predictions, and use these predictions as labels to learn a sparse linear model that is locally faithful. The weights of this linear model are used as 'feature importances'. Click the link below to learn more.",
      "mimicTitle": "Mimic (Global Surrogate Explanations)",
      "mimicDescription": "This explainer is based on the idea of training global surrogate models to mimic blackbox models. A global surrogate model is an intrinsically interpretable model that is trained to approximate the predictions of any black box model as accurately as possible. Feature importance values are model-based feature importance values of your underlying surrogate model (LightGBM, or Linear Regression, or Stochastic Gradient Descent, or Decision Tree)",
      "pfiTitle": "Permutation feature importance (PFI)",
      "pfiDescription": "This explainer randomly shuffles data one feature at a time for the entire dataset and calculates how much the performance metric of interest changes (default performance metrics: F1 for binary classification, F1 Score with micro average for multiclass classification and mean absolute error for regression). The larger the change, the more important that feature is. This explainer can only explain the overall behavior of the underlying model but does not explain individual predictions. Feature importance value of a feature represents the delta in the performance of the model by perturbing that particular feature."
    }
  },
  "ErrorAnalysis": {
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Feature {0}",
    "cells": "Cells",
    "cellsInfo": "The number of cells selected in the matrix filter.  When no cells selected, displays - (dash) and shows the global cohort information as if all cells are selected.",
    "cellsTitle": "Additional information on cells",
    "cohortInfo": "Cohort info",
    "correctPrediction": "Correct predictions",
    "incorrectPrediction": "Incorrect predictions",
    "whatIfDatapoints": "What-If datapoints",
    "allSelected": "All selected",
    "correctTotal": "Correct/Total",
    "dataExploration": "Dataset Exploration",
    "instanceView": "Instance View",
    "dataExplorerView": "Data explorer",
    "errorCoverage": "Error coverage",
    "errorCoverageInfo": "The error coverage shows the percentage of errors that are concentrated in the selection out of all errors present in the dataset.",
    "errorCoverageTitle": "Additional information on error coverage",
    "errorRate": "Error rate",
    "errorRateInfo": "The error rate represents the percentage of instances in the node for which the system has failed.",
    "errorRateTitle": "Additional information on error rate",
    "globalExplanationView": "Global explanation",
    "localExplanationView": "Local explanation",
    "noFeature": "No feature",
    "globalImportance": "Global Importance",
    "incorrectTotal": "Incorrect/Total",
    "treeMapDescription": "To retrain the tree map, select and save the features below. The feature importances were calculated using mutual information with the error on the true labels.  Please use it as a guideline for training the tree map.",
    "Cohort": {
      "cohort": "Cohort",
      "defaultLabel": "All data"
    },
    "CohortInfo": {
      "cohortInformation": "Cohort information",
      "saveCohort": "Save as a new cohort"
    },
    "EditCohort": {
      "subText": "Learn about the selected cohort. Edit its cohort name. Delete this cohort.",
      "cohortName": "Cohort name"
    },
    "FeatureList": {
      "importances": "Importances",
      "features": "Features"
    },
    "InspectionView": {
      "emptyError": "Please select datapoints in the incorrect and correct categories by clicking the checkboxes.",
      "selectedDatapoints": "Selected datapoints"
    },
    "InstanceView": {
      "selection": "{0} selected",
      "inspect": "Inspect"
    },
    "MainMenu": {
      "treeMap": "Tree map",
      "heatMap": "Heat map",
      "errorAnalysisLabel": "Error analysis",
      "errorExplorerLabel": "Error explorer:",
      "errorExplorer": "Error explorer",
      "fullscreen": "Fullscreen",
      "whatIf": "What-If",
      "featureList": "Feature list",
      "shiftCohort": "Shift cohort",
      "saveCohort": "Save cohort",
      "cohortList": "Cohort list",
      "cohortInfo": "Cohort info",
      "explanation": "Explanation",
      "cohortSettings": "Cohort settings"
    },
    "MapShift": {
      "subText": "Do you want to restart your exploration with a different map?  Save it as a new cohort.  Otherwise, your chosen filters will be lost.",
      "title": "Map shift",
      "close": "Close",
      "move": "Move",
      "saveAs": "Save as a new cohort",
      "shift": "Shift",
      "cancel": "Cancel"
    },
    "MatrixArea": {
      "emptyText": "Select two features by using the dropdowns above.  You can cluster and filter your data along two dimensions.",
      "selectAll": "Select all",
      "clearAll": "Clear all"
    },
    "MatrixLegend": {
      "heatMapDescription": "With the grid map you can focus on specific filters and combine error rates.  Start with two dataset features to compare."
    },
    "Navigation": {
      "errorExplorer": "Error explorer",
      "dataExplorer": "Data explorer",
      "globalExplanation": "Global explanation",
      "localExplanation": "Local explanation",
      "localExplanationInspection": "Local explanation (Inspection)"
    },
    "SaveCohort": {
      "subText": "Save the current cohort to the cohort list. You can revisit the saved cohort via the cohort list.",
      "save": "Save",
      "saveTitle": "Save as a new cohort",
      "cancel": "Cancel",
      "close": "Close",
      "move": "Move",
      "cohortName": "Cohort name"
    },
    "TreeView": {
      "treeDescription": "The tree visualization uses the mutual information between each feature and the error to best separate error instances from success instances hierarchically in the data.",
      "treeDescriptionExpanded": "This simplifies the process of discovering and highlighting common failure patterns. To find important failure patterns, look for nodes with a stronger red color (i.e., high error rate) and a higher fill line (i.e., high error coverage). To edit the list of features being used in the tree go to the \"Feature list\" panel."
    },
    "WhatIfPanel": {
      "whatIfHeader": "What-If"
    }
  },
  "Fairness": {
    "loremIpsum": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.",
    "defaultClassNames": "Class {0}",
    "defaultFeatureNames": "Sensitive feature {0}",
    "defaultSingleFeatureName": "Sensitive feature",
    "defaultCustomMetricName": "Custom metric {0}",
    "performanceTab": "Fairness in Performance",
    "opportunityTab": "Fairness in Opportunity",
    "modelComparisonTab": "Model comparison",
    "tableTab": "Detail view",
    "dataSpecifications": "Data statistics",
    "attributes": "Attributes",
    "singleAttributeCount": "1 sensitive feature",
    "attributesCount": "{0} sensitive features",
    "instanceCount": "{0} instances",
    "close": "Close",
    "done": "Done",
    "calculating": "Calculating...",
    "performanceMetricLegacy": "Performance metric",
    "sensitiveFeatures": "01 Sensitive features",
    "performanceMetric": "02 Performance metrics",
    "fairnessMetric": "03 Fairness metrics",
    "errorOnInputs": "Error with input. Sensitive features must be categorical values at this time. Please map values to binned categories and retry.",
    "Performance": {
      "header": "How do you want to measure performance?",
      "modelMakes": "model makes",
      "modelsMake": "models make",
      "body": "Your data contains {0} labels and your {2} {1} predictions. Based on that information, we recommend the following metrics. Please select one metric from the list.",
      "binaryClassifier": "binary classifier",
      "probabilisticRegressor": "probit regressor",
      "regressor": "regressor",
      "binary": "binary",
      "continuous": "continuous"
    },
    "Fairness": {
      "pickerHeader": "How do you want to measure fairness?",
      "header": "Fairness measured in terms of disparity",
      "bodyLegacy": "Fairness metrics quantify variation of your model's behavior across selected features. There are two of fairness metrics: more to come....",
      "body": "Fairness metrics quantify variation of your model's behavior across selected features. There are several kinds of fairness metrics that are based on a variety of performance metrics. They either capture the difference or ratio between the extreme values across the groups, or simply the worst value of any group."
    },
    "Header": {
      "title": "Fairness",
      "documentation": "Documentation"
    },
    "Footer": {
      "back": "Back",
      "next": "Next"
    },
    "Intro": {
      "welcome": "Welcome to the",
      "fairnessDashboard": "Fairness dashboard",
      "introBody": "The fairness dashboard enables you to assess tradeoffs between performance and fairness of your models",
      "explanatoryStep": "To set up the assessment, you need to specify a sensitive feature, a performance metric, and a fairness metric.",
      "getStarted": "Get started",
      "features": "Sensitive features",
      "featuresInfo": "Sensitive features are used to split your data into groups. Fairness of your model across these groups is measured by fairness metrics. Fairness metrics quantify how much your model's behavior varies across these groups.",
      "performance": "Performance metric",
      "performanceInfo": "Performance metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. The difference between the extreme values of the performance metric across the groups is reported as the disparity in performance.",
      "fairness": "Fairness metrics",
      "fairnessInfo": "Fairness metrics are used to evaluate the overall quality of your model as well as the quality of your model in each group. Fairness metrics may represent the difference or ratio between the extreme values of a performance metric, or simply the worst value of any group."
    },
    "ModelComparison": {
      "title": "Model comparison",
      "howToRead": "How to read this chart",
      "lower": "lower",
      "higher": "higher",
      "howToReadText": "This chart represents each of the {0} models as a selectable point. The x-axis represents {1}, with {2} being better. The y-axis represents disparity, with lower being better.",
      "insightsLegacy": "Insights",
      "insights": "Key insights",
      "downloadReport": "Download report",
      "disparity": " The disparity ",
      "rangesFrom": " ranges from ",
      "to": " to ",
      "period": ". ",
      "introModalText": "Each model is a selectable point. Click or tap on a model for its full fairness assessment.",
      "helpModalText1": "The x-axis represents performance, with {0} being better.",
      "helpModalText2": "The y-axis represents fairness metric values, with {0} being better.",
      "insightsText2": "{0} ranges from {1} to {2}. {3} ranges from {4} to {5}.",
      "insightsText3": "The model with the best performance achieves {0} of {1} and {2} of {3}.",
      "insightsText4": "The model with the best fairness metric value achieves {0} of {1} and {2} of {3}.",
      "insightsText3v1FairnessMetric": "a disparity",
      "disparityInOutcomes": "Disparity in predictions",
      "disparityInPerformance": "Disparity in {0}",
      "howToMeasureDisparity": "How should disparity be measured?"
    },
    "Report": {
      "modelName": "Model {0}",
      "title": "Disparity in performance",
      "globalPerformanceText": "Is the overall {0}",
      "performanceDisparityText": "Is the disparity in {0}",
      "editConfiguration": "Edit configuration",
      "backToComparisonsLegacy": "Multimodel view",
      "backToComparisons": "Back to all models",
      "assessmentResults": "Assessment results for",
      "performanceChartHeaderBinaryClassification": "False positive and false negative rates",
      "performanceChartHeaderProbability": "Over- and underprediction",
      "performanceChartHeaderRegression": "Distribution of errors",
      "outcomesTitle": "Disparity in predictions",
      "expandSensitiveAttributes": "Expand sensitive attributes",
      "collapseSensitiveAttributes": "Collapse sensitive attributes",
      "minTag": "Min",
      "maxTag": "Max",
      "groupLabel": "Subgroup",
      "overallLabel": "Overall",
      "underestimationError": "Underprediction",
      "underpredictionExplanation": "(predicted = 0, true = 1)",
      "overpredictionExplanation": "(predicted = 1, true = 0)",
      "overestimationError": "Overprediction",
      "falseNegativeRate": "False negative rate",
      "falsePositiveRate": "False positive rate",
      "classificationOutcomesHowToRead": "The bar chart shows the selection rate in each group, meaning the fraction of points classified as 1.",
      "regressionOutcomesHowToRead": "Box plots show the distribution of predictions in each group. Individual data points are overlaid on top.",
      "classificationPerformanceHowToReadV2": "The bar chart shows the false negative and false positive rates in each group.",
      "classificationPerformanceHowToRead1": "The bar chart shows the distribution of errors in each group.",
      "classificationPerformanceHowToRead2": "Errors are split into overprediction errors (predicting 1 when the true label is 0), and underprediction errors (predicting 0 when the true label is 1).",
      "classificationPerformanceHowToRead3": "The reported rates are obtained by dividing the number of errors by the overall group size.",
      "probabilityPerformanceHowToRead1": "The bar chart shows mean absolute error in each group, split into overprediction and underprediction.",
      "probabilityPerformanceHowToRead2": "On each example, we measure the difference between the prediction and the label. If it is positive, we call it overprediction and if it is negative, we call it underprediction.",
      "probabilityPerformanceHowToRead3": "We report the sum of overprediction errors and the sum of underprediction errors divided by the overall group size.",
      "regressionPerformanceHowToRead": "Error is the difference between the prediction and the label. Box plots show the distribution of errors in each group. Individual data points are overlaid on top.",
      "distributionOfPredictions": "Distribution of predictions",
      "distributionOfErrors": "Distribution of errors",
      "tooltipPrediction": "Prediction: {0}",
      "tooltipError": "Error: {0}",
      "chartChoiceDropdownHeader": "Charts"
    },
    "Feature": {
      "header": "Along which features would you like to evaluate your model's fairness?",
      "body": "Fairness is evaluated in terms of disparities in your model's behavior. We will split your data according to values of each selected feature, and evaluate how your model's performance metric and predictions differ across these splits.",
      "learnMore": "Learn more",
      "summaryCategoricalCount": "This feature has {0} unique values",
      "summaryNumericCount": "This numeric feature ranges from {0} to {1}, and is grouped into {2} bins.",
      "showCategories": "Show all",
      "hideCategories": "Collapse",
      "categoriesOverflow": "   and {0} additional categories",
      "editBinning": "Edit groups",
      "subgroups": "Subgroups"
    },
    "Metrics": {
      "accuracyScore": "Accuracy",
      "precisionScore": "Precision",
      "recallScore": "Recall",
      "zeroOneLoss": "Zero-one loss",
      "specificityScore": "Specificity score",
      "missRate": "Miss rate",
      "falloutRate": "Fallout rate",
      "maxError": "Max error",
      "meanAbsoluteError": "Mean absolute error",
      "meanSquaredError": " Mean squared error",
      "meanSquaredLogError": "Mean squared log error",
      "medianAbsoluteError": "Median absolute error",
      "average": "Average prediction",
      "selectionRate": "Selection rate",
      "overprediction": "Overprediction",
      "underprediction": "Underprediction",
      "falsePositiveRate": "False positive rate",
      "falseNegativeRate": "False negative rate",
      "r2_score": "R-squared score",
      "rms_error": "Root mean squared error",
      "auc": "Area under ROC curve",
      "balancedRootMeanSquaredError": "Balanced root mean squared error",
      "balancedAccuracy": "Balanced accuracy",
      "f1Score": "F1-score",
      "logLoss": "Log loss",
      "accuracyDescription": "The fraction of data points classified correctly.",
      "precisionDescription": "The fraction of data points classified correctly among those classified as 1.",
      "recallDescription": "The fraction of data points classified correctly among those whose true label is 1. Alternative names: true positive rate, sensitivity.",
      "rmseDescription": "Square root of the average of squared errors.",
      "mseDescription": "The average of squared errors.",
      "meanAbsoluteErrorDescription": "The average of absolute values of errors. More robust to outliers than MSE.",
      "r2Description": "The fraction of variance in the labels explained by the model.",
      "f1ScoreDescription": "F1-score is the harmonic mean of precision and recall.",
      "aucDescription": "The quality of the predictions, viewed as scores, in separating positive examples from negative examples.",
      "balancedRMSEDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "balancedAccuracyDescription": "Positive and negative examples are reweighted to have equal total weight. Suitable if the underlying data is highly imbalanced.",
      "falsePositiveRateDescription": "The fraction of data points classified incorrectly among those whose true label is 0.",
      "falseNegativeRateDescription": "The fraction of data points classified incorrectly among those whose true label is 1.",
      "accuracyScoreDifference": "Accuracy score difference",
      "accuracyScoreDifferenceDescription": "The maximum difference in accuracy score between any two groups.",
      "accuracyScoreMin": "Minimum accuracy score",
      "accuracyScoreMinDescription": "The minimum accuracy score of all groups.",
      "accuracyScoreRatio": "Accuracy score ratio",
      "accuracyScoreRatioDescription": "The minimum ratio in accuracy score between any two groups.",
      "balancedAccuracyScoreMin": "Balanced accuracy score minimum",
      "balancedAccuracyScoreMinDescription": "The minimum of all groups' average recall on each class (0 and 1).",
      "demographicParityDifference": "Demographic parity difference",
      "demographicParityDifferenceDescription": "The maximum difference in selection rate, that is the fraction with predicted label 1, between any two groups.",
      "demographicParityRatio": "Demographic parity ratio",
      "demographicParityRatioDescription": "The minimum ratio of selection rates, that is the fraction with predicted label 1, between any two groups.",
      "equalizedOddsDifference": "Equalized odds difference",
      "equalizedOddsDifferenceDescription": "Either the maximum difference between true positive rates of any two groups or the maximum difference between false positive rates of any two groups, whichever is greater.",
      "equalizedOddsRatio": "Equalized odds ratio",
      "equalizedOddsRatioDescription": "Either the minimum ratio between true positive rates of any two groups or the minimum ratio between false positive rates of any two groups, whichever is smaller.",
      "trueNegativeRateDifference": "True negative rate difference",
      "errorRateDifference": "Error rate difference",
      "errorRateDifferenceDescription": "The maximum difference between the error rates of any two groups.",
      "errorRateRatio": "Error rate ratio",
      "errorRateRatioDescription": "The minimum ratio between the error rates of any two groups.",
      "f1ScoreMin": "Minimum F1-score",
      "f1ScoreMinDescription": "The minimum F1-score of all groups.",
      "falseNegativeRateDifference": "False negative rate difference",
      "falseNegativeRateDifferenceDescription": "The maximum difference between false negative rates of any two groups. Also sometimes referred to as miss rate difference.",
      "falseNegativeRateRatio": "False negative rate ratio",
      "falseNegativeRateRatioDescription": "The minimum ratio between false negative rates of any two groups. Also sometimes referred to as miss rate ratio.",
      "falsePositiveRateDifference": "False positive rate difference",
      "falsePositiveRateDifferenceDescription": "The maximum difference between false positive rates of any two groups. Also sometimes referred to as fall-out difference.",
      "falsePositiveRateRatio": "False positive rate ratio",
      "falsePositiveRateRatioDescription": "The minimum ratio between false positive rates of any two groups. Also sometimes referred to as fall-out ratio.",
      "logLossMax": "Maximum log loss",
      "logLossMaxDescription": "The maximum logistic loss of all groups.",
      "meanAbsoluteErrorMax": "Maximum mean absolute error",
      "meanAbsoluteErrorMaxDescription": "The maximum mean absolute error of all groups.",
      "meanSquaredErrorMax": "Maximum mean squared error",
      "meanSquaredErrorMaxDescription": "The maximum mean squared error of all groups.",
      "precisionScoreMin": "Minimum precision score",
      "precisionScoreMinDescription": "The minimum precision score of all groups.",
      "r2ScoreMin": "Minimum R2-score",
      "r2ScoreMinDescription": "The minimum R2-score of all groups.",
      "recallScoreMin": "Minimum recall score",
      "recallScoreMinDescription": "The minimum recall score of all groups.",
      "ROCAUCScoreMin": "Minimum ROC AUC score",
      "ROCAUCScoreMinDescription": "The minimum Area Under the Receiver Operating Characteristic Curve (ROC AUC) of all groups.",
      "trueNegativeRateDifferenceDescription": "The maximum difference between true negative rates of any two groups. Also sometimes referred to as specificity score difference.",
      "trueNegativeRateRatio": "True negative rate ratio",
      "trueNegativeRateRatioDescription": "The minimum ratio between true negative rates of any two groups. Also sometimes referred to as specificity score ratio.",
      "truePositiveRateDifference": "True positive rate difference",
      "truePositiveRateDifferenceDescription": "The maximum difference between true positive rates of any two groups. Also sometimes referred to as equal opportunity difference or recall score difference.",
      "truePositiveRateRatio": "True positive rate ratio",
      "truePositiveRateRatioDescription": "The minimum ratio between true positive rates of any two groups. Also sometimes referred to as equal opportunity ratio or recall score ratio.",
      "Groups": {
        "equalizedOdds": "Equalized odds",
        "classificationAccuracyAndErrorRate": "Accuracy / error rate",
        "regressionError": "Error",
        "selectionRate": "Demographic Parity / Selection rate",
        "trueNegativeRate": "True negative rate / specificity",
        "truePositiveRate": "True positive rate / recall / sensitivity",
        "falseNegativeRate": "False negative rate / miss rate",
        "falsePositiveRate": "False positive rate / fall-out",
        "precision": "Precision",
        "overUnderPrediction": "Over- and underprediction",
        "auc": "Area under ROC curve",
        "average": "Average prediction",
        "f1_score": "F1-score",
        "loss": "Loss",
        "r2_score": "R-squared score",
        "custom": "Custom metrics"
      }
    },
    "BinDialog": {
      "header": "Configure bins",
      "makeCategorical": "Treat as categorical",
      "save": "Save",
      "cancel": "Cancel",
      "numberOfBins": "Number of bins:",
      "categoryHeader": "Bin values:"
    },
    "DropdownHeaders": {
      "sensitiveFeature": "Sensitive feature",
      "performanceMetric": "Performance metric",
      "fairnessMetric": "Fairness metric"
    }
  },
  "ModelAssessment": {
    "Navigation": {
      "modelStatistics": "Model statistics"
    }
  }
}